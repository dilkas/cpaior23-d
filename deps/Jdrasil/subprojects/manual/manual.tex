\documentclass[a4paper, ukenglish, twoside, openright]{jdrasilmanual}

\setversion{0.4-dev}
\author{Max Bannach, Sebastian Berndt, Thorsten Ehlers}

\begin{document}

\part{Getting Started}
%\chapter{About \Jdrasil}
\section{Technical Overview and Design Principles}
\Jdrasil{} is a modular Java library for computing tree
decompositions both, exact and heuristically. The goal of \Jdrasil\ is
to allow other projects to add tree decompositions to their
applications as easy as possible. In order to achieve this, \Jdrasil\
is designed in a very modular way: Every algorithm is implemented as
interchangeable as possible. At the same time, algorithms are
implemented in a clean object oriented manner, making it easy to
understand and extend the implementation. We hope that this approach
makes it easier to study the practical aspects of tree width
algorithms, and that it helps to push the practical usage of tree
decompositions.

To make the usage of \Jdrasil\ as easy as possible, the whole library can be
compiled and used without any dependencies. Everything boils down to a
single, platform independent \texttt{jar} file that can freely be used
with the mentioned copyright.

However, this design principle comes with the price of algorithms that
are not ``on the edge optimized''. We believe that this is not a
problem for most applications, as the algorithms run in the same
principle time complexity anyway. If we, however, wish to use super
optimized algorithms, \Jdrasil{} provides the concept of
\emph{upgrades}. An upgrade is a piece of third party software
(e.\,g., another Java library, an optimized C implementation of an
algorithm, a SAT solver, \dots) that can be used to \emph{replace} some
functionality of \Jdrasil. An upgrade will usually boost the
performance of \Jdrasil\ in general, or on a specific target platform
(for instance on a parallel machine), and will add dependencies
(\Jdrasil\ with upgrades may not be platform
independent). Nevertheless, an upgrade will \emph{not} increase the
functionality of \Jdrasil, only the performance and, hence, a scripted
based on an upgraded version of \Jdrasil\ is still platform
independent.

\section{Contributors}
The core developer of \Jdrasil\ are (in lexicographical order):
\begin{itemize}
  \item Max Bannach
  \item Sebastian Berndt
  \item Thorsten Ehlers
\end{itemize}
\section{How to Cite \Jdrasil}
If you use \Jdrasil\ for your project, please refer to our GitHub
page and respect the copyright. If you use \Jdrasil\ for a research
project, please cite our introduction paper:
\begin{lstlisting}[language=BibTeX]
  @inproceedings{jdrasil,
    author = {Max Bannach and Sebastian Berndt and Thorsten Ehlers},
    title = {Jdrasil: A Modular Library for Computing Tree Decompositions},
    booktitle = {Experimental Algorithms - 16th International Symposium, {SEA} 2017,
               London, England, June 21 -- 23, 2017, Proceedings},
    year = {2017}
  }
\end{lstlisting}
\chapter{Build and Run \Jdrasil}
\section{Obtain and Use the Latest Version}
The latest version of \Jdrasil{} can be obtained from
\url{https://maxbannach.github.io/Jdrasil/}. The obtained
\texttt{.jar} file is executable, i.\,e., if \Jdrasil\ should be used
as standalone solver, we can simple use:
\begin{lstlisting}[language=bash]
  # this will execute the exact mode
  java -jar Jdrasil.jar
\end{lstlisting}
or alternatively
\begin{lstlisting}[language=bash]
  java -cp Jdrasil.jar jdrasil.Exact
\end{lstlisting}
In addition, we can execute \Jdrasil\ in the heuristic or
approximation mode:
\begin{lstlisting}[language=bash]
  java -cp Jdrasil.jar jdrasil.Heuristic
  java -cp Jdrasil.jar jdrasil.Approximation
\end{lstlisting}
Note that the heuristic mode tries to improve the solution
until a \texttt{SIGTERM} is received, i.\,e., \Jdrasil\ has to be stopped
manually in this mode. 

With the same \texttt{.jar} file, \Jdrasil\ can also be used as
library: simply add the \texttt{.jar} to the classpath of the desired
project. For instance, the following simple program uses \Jdrasil\ to
compute an exact tree decomposition:
\begin{lstlisting}[language=Java]
  import jdrasil.graph.*;
  import jdrasil.algorithms.*;

  public class Main {
    public static void main(String[] args) {
      // create your own graph
      Graph<Integer> G = GraphFactory.emptyGraph();
      for (int v = 1; v <= 5; v++) { G.addVertex(v); }
      G.addEdge(1, 2);
      G.addEdge(1, 4);
      G.addEdge(2, 3);
      G.addEdge(2, 4);
      G.addEdge(4, 5);

      // output graph in PACE format
      System.out.println(G);
      
      // compute exact decomposition
      TreeDecomposition<Integer> td = null;
      try {
        td = new ExactDecomposer<Integer>(G).call();
      } catch (Exception e) {
        System.out.println("something went wrong");
      }

      // ouput tree decomposition in PACE format
      System.out.println(td);
    }
  }
\end{lstlisting}
If the program is stored in a file \file{Main.java}, it can be
compiled and used with the following commands:
\begin{lstlisting}[language=bash]
  javac -cp Jdrasil.jar: Main.java
  java -cp Jdrasil.jar: Main
\end{lstlisting}
\section{Program Arguments}
The behavior of \Jdrasil\ can be influenced by program arguments. The
following tables provides an overview of all available arguments:
\begin{center}
  \def\check{\textcolor{jdrasil.fg}{\checkmark}}
  \begin{longtable}{lp{5cm}ccc}
    \toprule
    Argument & Effect & Exact & Heuristic & Approximation\\
    \cmidrule(lr){1-5}
    -h & prints a help dialog & \check & \check & \check \\
    %
    -s <seed> & seeds the RNG & \check & \check & \check\\
    % 
    -parallel & enables parallel processing (the
    \JClass{GraphSplitter} will split and handle atoms in parallel)
    & \check &  &
    \check\\
    % 
    -instant & heuristic will output a solution as fast as possible &
    & \check & \\
    %
    -log & prints log information during computation & \check & \check
    & \check \\
    \bottomrule
  \end{longtable}
\end{center}
Some of the arguments may only make sense if \Jdrasil\ is used as
stand alone program, others may also be useful in an scenario where
\Jdrasil\ is used as a library. For instance, the argument ``-parallel''
modifes some algorithms to run in parallel, this is useful in other
applications as well. \Jdrasil\ handles such properties in the class
\JClass{JdrasilProperties} and to enable an property ``-x'' (for
instance ``-parallel'') we can do from source:
\begin{lstlisting}[language=Java]
  // set value of property "x" to "some value"
  JdrasilProperties.setProperty("x", "some value");
  // just add property "parallel"
  JdrasilProperties.setProperty("parallel", "");
\end{lstlisting}
Note that some properties (as the random seed) need an actual value,
this is the second argument in the example code above.
\section{Build \Jdrasil\ from Source}
\Jdrasil\ uses Gradle\footnote{\url{www.gradle.org}} as building
tool. Thereby it takes advantage of the Gradle wrapper: 
in order to build \Jdrasil\ the only requirements are an up-to-date
\texttt{JDK}\footnote{\Jdrasil\ needs at least Java $8$.} and an active internet connection. The Gradle wrapper
will download everything needed by itself.

To get started, we need the latest version of \Jdrasil\ and switch to
its root directory:
\begin{lstlisting}[language=bash]
  git clone https://github.com/maxbannach/Jdrasil.git
  cd Jdrasil
\end{lstlisting}
The folder contains a directory \texttt{subprojects}, which contains
the source code of \Jdrasil. Besides this, there are a couple of Gradle
files, most of which do not require our attention. The only important
file is \lstinline{gradlew} (or \lstinline{gradlew.bat} on
Windows). This is the Gradle wrapper that we will use to build \Jdrasil.
In order to use it, we have to execute on of the following commands
(depending on our operating system):
\begin{lstlisting}[language=bash]
  # on Unix
  ./gradlew <task>
  # or, if gradlew is not executable
  sh gradlew <task>
  # on Windows
  gradlew <task>
\end{lstlisting}
In the rest of the manual, we will use the syntax for an Unix
system. But everything can be done in the same way on a Windows machine.

\subsection{Build the Executable and Library}
To compile the core source code of \Jdrasil\ we use one of the following
commands:
\begin{lstlisting}[language=bash]
  # just build
  ./gradlew assemble
  # or build and test
  ./gradlew build
\end{lstlisting}
This will create a directory \texttt{build}, containing the
directories \texttt{classes/main} and \texttt{jars}. The first
directory will contain the compiled class files, the second will
contain \texttt{Jdrasil.jar}.

In order to run the freshly build \Jdrasil, we can do one of the
following:
\begin{lstlisting}[language=bash]
  java -jar build/jars/Jdrasil.jar
  java -cp build/jars/Jdrasil.jar jdrasil.Exact
  java -cp build/classes/main jdrasil.Exact
\end{lstlisting}
To use the present build of \Jdrasil\ as library, we can simply add
the created \texttt{.jar} file to the classpath of our desired
project.
\section{Build the Documentation}
The documentation of \Jdrasil\ consists of two parts: the classic
JavaDocs, which provide detailed information about the individual
classes, and this manual, which provides an high level view on some
design principles used by \Jdrasil. The JavaDocs can, without further
requirements, be build by the following command:
\begin{lstlisting}[language=bash]
  ./gradlew javadoc
\end{lstlisting}
This will create the folder \texttt{builds/docs/javadoc} containing
the documentation.

This manual is written in \LaTeX\ and in order to compile it, an
up-to-date \LuaLaTeX\ installation must be available. If this is the
case, the manual can be typeset by
\begin{lstlisting}[language=bash]
  ./gradlew manual
\end{lstlisting}
This command will place this manual as \texttt{.pdf} file in
\texttt{builds/docs/manual}.
\section{Installing Upgrades}
As mentioned earlier, \Jdrasil\ can be build and used without any
dependencies. This makes it easy to update, distribute, and use
\Jdrasil. However, sometimes third-party software may provide a
significant speed up in the process of solving some subproblems. In
such scenarios, the speed of \Jdrasil\ can be improved by
\emph{upgrades}. This topic will be discussed in detail in
section~\ref{part:upgrades}.

All upgrades have in common, that \Jdrasil\ uses the following
convention to build and use them. To $\{\text{download}, \text{build},
\text{install}\}$ (depending on the upgrade) an upgrade $x$, we can
execute the following command:
\begin{lstlisting}[language=bash]
  ./gradlew upgrade_x
\end{lstlisting}
Which will $\{\text{download}, \text{build},
\text{install}\}$ the required files and place them in
\texttt{build/upgrades}.
To run \Jdrasil\ with the installed upgrade, either do (if the upgrade is
a Java library):
\begin{lstlisting}[language=bash]
  java -cp build/jars/Jdrasil.jar:build/upgrades/x.jar  jdrasil.Exact
\end{lstlisting}
or (if the upgrade is a native library):
\begin{lstlisting}[language=bash]
  java -Djava.library.path=build/upgrades -jar build/jars/Jdrasil.jar
\end{lstlisting}
The start scripts of \Jdrasil\ (see next section) will automatically look for upgrades in
these locations.
\section{Building Startscripts (not only ) for PACE}
As \Jdrasil\ was developed for the \emph{Parameterized Algorithms and
  Computational Experiments Challenge} (PACE) \cite{pace} in the first place, it
naturally provides the interfaces required by PACE. To build them, we
can simply use:
\begin{lstlisting}[language=bash]
  ./gradlew pace
\end{lstlisting}
This will, if not already done, build the Java files and will place
two shell scripts in the root directory: \texttt{tw-exact} and
\texttt{tw-heuristic}. The first script will execute \Jdrasil\ in
exact mode (meaning it reads an graph from stdin, computes an optimal
decomposition, and prints it on stdout); while the second script runs
\Jdrasil\ in heuristic mode, meaning it will run in an infinite loop
trying to heuristically find a good decomposition~–~the decomposition
will be printed if a SIGINT or SIGTERM is received. Example usage is:
\begin{lstlisting}[language=bash]
  ./tw-exact -s 42 < myGraph.gr > myGraph.td
  ./tw-heuristic -s 42 < myHugeGraph.gr > myHugeGraph.gr.td
\end{lstlisting}
For more details about the usage of these scripts, take a look at the
PACE website: \url{https://pacechallenge.wordpress.com/pace-2017/track-a-treewidth/}.

These scripts are available both, as Shell script for UNIX systems and
as \texttt{.bat} script for Windows. They can also be build directly
via:
\begin{lstlisting}[language=bash]
  ./gradlew exact
  ./gradlew heuristic
\end{lstlisting}
Finally, there are also scripts available for running \Jdrasil\ with
approximation algorithms:
\begin{lstlisting}[language=bash]
  ./gradlew approximation
\end{lstlisting}
This will generate \texttt{tw-approximation}, which can be used as the
scripts from above.

\part{Graphs}
Since \Jdrasil\ is a tool for computing tree decompositions, it
implements a variety of graph algorithms. Hence, many graphs and
``graph objects'' will be handed throughout the library. It is,
therefore, worth to spend some time and study the design principles
\Jdrasil\ follows when it handles graphs.

All classes and methods that are designed to deal with graphs directly
are stored in the package \JClass{jdrasil.graph}. One (not that small)
exception is the collection of algorithms and procedures that are
meant to compute tree decompositions~–~as this is \Jdrasil's main
task, they obtain some extra packages. Within the graph package, the
working horse is the class \JClass{Graph} which represents all graphs
that occur within \Jdrasil. The following sections will capture the
design of this class, how we can obtain and store objects of the
class, and how we can modify existing \JClass{Graph}
objects. Furthermore, we will discuss how \Jdrasil\ implements
algorithms for computing graph properties and invariants, and, most
importantly, how tree decompositions are managed.

\chapter{The Graph Class}
The work horse of \Jdrasil's graph engine is the class
\JClass{jdrasil.graph.Graph}. There are a couple of design decisions
that were made during the development of this class, which grant some
advantages in the context of computing tree decompositions, but which
may result in disadvantages in other algorithmic tasks. We will
discuss these implementation details in the following. 

An object of the \JClass{Graph} class represents a directed graph
$G=(V, E)$ where $V$ is a set of \emph{arbitrary objects}, and
$E\subseteq V\times V$. An undirected graph is represented as directed
graph with a symmetric edge relation. Since, in the context of
computing tree decompositions, we will often deal with undirected
graphs, the class provides most methods in an additional symmetric
implementation, so that the \JClass{Graph} class can be used to work
with undirected graphs in a natural way.

As stated above, 
\note{This generic approach nevertheless allows the usage of vertex
  classes. Indeed, even the vertex class from, say another library,
  can be plugged in.}
the \JClass{Graph} represents a directed graph where
$V$ is a set of arbitrary objects.  Consequently, there is no vertex
or node class in \Jdrasil, instead, the \JClass{Graph} class is
generic and \emph{any class} can be used as vertex type~–~the only
restriction is that the class has a natural order, i.\,e., it is
\emph{comparable}. For instance, the following graphs are of the types
\JClass{Integer} and \JClass{Character}, respectively.
\begin{center}
\begin{tikzpicture}

\graph[spring electrical layout, edges = {-latex, semithick}, node distance=0.75cm] {
  1 -> {2,3,4};
  2 -> 3;
  4 -> {5, 6 -> 1};
};

\graph[spring electrical layout, edges = {-latex, semithick}, node
distance=0.75cm, anchor at = {(5,0)}] {
  A -> {B,C,D};
  B -> C;
  D -> {E, F -> A};
};
\end{tikzpicture}
\end{center}
We could create the graph from above with any other comparable Java
class; and also run all the algorithms implemented by \Jdrasil\ on
it. Especially, for a class representing subsets of vertices, we can
represent the following graphs:
\begin{center}
\begin{tikzpicture}

\graph[spring electrical layout, edges = {semithick}, node distance=1.5cm] {
  x / "$\{\,1,2,3\,\}$";
  y / "$\{\,1,4,6\,\}$";
  z / "$\{\,4,5\,\}$";
  x --[orient=0] y -- z;
};

\graph[spring electrical layout, edges = {semithick}, node
distance=1.5cm, anchor at = {(5.5,0)}] {
  x / "$\{$\,A,B,C\,$\}$";
  y / "$\{$\,A,D,F\,$\}$";
  z / "$\{$\,D,E\,$\}$";
  x --[orient=0] y -- z;
};
\end{tikzpicture}
\end{center}
This is essentially the way \Jdrasil\ represents tree decompositions,
see Chapter~\ref{chapter:treedecompositions} for more details.

The second part we mentioned earlier is that we simply consider $E$ as
$E\subseteq V\times V$, and in fact, \Jdrasil\ does not have a class
representing an edge. The \JClass{Graph} class only represents
adjacency information about its stored vertices. This makes working
with the \JClass{Graph} class more natural in many situations and
keeps algorithms simple. In the few cases where an edge class would be
useful, for instance if we work with weighted graphs, we interpreted
the edge label function, say $\lambda\colon E\rightarrow\Sigma$, as
$\lambda\colon V\times V\rightarrow\Sigma\cup\{\bot\}$, where we have
$\lambda(u,v)=\bot\Leftrightarrow (u,v)\not\in E$.

\section{Implementation Details}
In the core of the \JClass{Graph} class, the graph is stored as
adjacency list. Hence, we may iterate over the vertex and edge set in
time $O(|V|+|E|)$. Furthermore, the edge relation is stored in an hash
map, allowing us to perform adjacency tests in $O(1)$.

Adding vertices to the graph is performed in $O(1)$ as well, adding an
edge $(u,v)$ costs $O(\delta(v))$\footnote{$\delta(v)$ denotes the
  \emph{degree} of $v$: the number of nodes connected to $v$.}.  Adding directed edges actually is
(practically) faster, as this realized by array manipulation. When an
undirected edge is added, however, this time bound is strict, as some
additional information about the neighborhood is gathered. This
information can be used to get $O(1)$ access to some vertex
properties, for instance, testing if a vertex is simplicial, or
computing the fill-in value of a vertex.

\section{Working with Graphs}
To work with graphs, we first of all need one. Objects of the class
\JClass{Graph} are generated by the \JClass{GraphFactory}~–~a simple
empty graph can be obtained by:
\begin{lstlisting}[language=Java]
  Graph<Integer> myGraph = GraphFactory.emptyGraph();
\end{lstlisting}
Vertices can be added by the method \JMethod{Graph.addVertex}, or by
directly adding edges. The following two code fragments are
equivalent:
\begin{lstlisting}[language=Java]
  // variant 1
  myGraph.addVertex(1);
  myGraph.addVertex(2);
  myGraph.addEdge(1, 2);

  // or simply
  myGraph.addEdge(1, 2);
\end{lstlisting}
The\note{Since \Jdrasil\ mainly works on undirected graphs, methods
  for directed graphs are marked with \emph{directed}, while the
  methods for undirected graph have no prefix.} above code constructs the \emph{undirected} graph
\tikz[baseline={([yshift=-.5ex]1.center)}]\graph[edges={semithick}]{1--2};;
to create the \emph{directed} graph
\tikz[baseline={([yshift=-.5ex]1.center)}]\graph[edges={-latex,
  semithick}]{1->2};, we can use the function \JMethod{Graph.addDirectedEdge}:
\begin{lstlisting}[language=Java]
  // directed version
  myGraph.addDirectedEdge(1, 2);
\end{lstlisting}
We can also mix the commands and create a mixed graph. However,
usually, and in the following, we will stick to undirected graphs.

As we have added them, we can also remove vertices and edges from the
graph:
\begin{lstlisting}[language=Java]
  // removes the vertex and all incident edges, i.e., the one from 1 to 2
  myGraph.removeVertex(1);

  // removes the specific undirected edge
  myGraph.removeEdge(2, 3);
\end{lstlisting}
Removing an undirected edge will remove both directed edges, even if
only one of them is present (i.\,e., if the edge is actually
directed). Concretely, deleting a directed edge can be done by:
\begin{lstlisting}[language=Java]
  // directed version
  myGraph.removeDirectedEdge(1, 2);
\end{lstlisting}
\subsection{Iterating Over the Graph}
Many graph algorithms have to iterate over the vertices and edges of
the graph. The \JClass{Graph} implements an iterator for its vertices,
so to iterate over the vertices we can simply do (here, the graph has
vertices of type \JClass{Integer}):
\begin{lstlisting}[language=Java]
  for (Integer v : myGraph) {
    // do something with v
  }
\end{lstlisting}
As \Jdrasil\ does not know edge objects, there is no direct iteration
over edges. Instead, the neighborhood of a vertex is iterable as well.
In the directed case, this is straight forward:
\begin{lstlisting}[language=Java]
  for (Integer v : myGraph) {
    for (Integer w : myGraph.getNeighborhood(v)) {
      // do something with the edge (v,w)
    }
  }
\end{lstlisting}
In an undirected graph, however, we would iterate over every edge
twice (remember that an undirected graph is an directed graph with
symmetric edge relation). To overcome this issue, we can only pick the edge
in which the first vertex is lexicographical smaller:
\begin{lstlisting}[language=Java]
  for (Integer v : myGraph) {
    for (Integer w : myGraph.getNeighborhood(v)) {
      if (v.compareTo(w) > 0) continue; // skip second edge
      // do something with the undirected edge {v,w}
    }
  }
\end{lstlisting}
\subsection{Some Special Methods}
While most parts of the \JClass{Graph} class are kept as general as
possible, there are some special methods, which are designed towards
the computation of tree decompositions. They are defined for
\textcolor{jdrasil.fg}{undirected graphs} only, and will produce
mixed graphs or undefined results on directed graphs. These methods
are:
\begin{enumerate}
  \item\lstinline[language=Java]{contract(T v, T w)}: Contracts the
    \emph{undirected} edge $\{v,w\}$ into the vertex $v$. This will
    connect all edges incident to $w$ to $v$~–~this will, however,
    \emph{not} create multi-edges. This method will return a
    \JClass{ContractionInformation} object, which can be used to
    revert the contraction.
 \item\lstinline[language=Java]{deContract(ContractionInformation info)}: Will revert the contraction of an edge.
  \item\lstinline[language=Java]{eliminateVertex(T v)}: Eliminating
    a vertex $v$ will a) turn $N(v)$\footnote{$N(v)$ denotes the
      \emph{neighbourhood} of $v$: all nodes connected to $v$.} into a clique, and b) remove $v$
    from the graph. This method will return an
    \JClass{EliminationInformation} object, which can be used to
    revert the elimination.
  \item\lstinline[language=Java]{deEliminateVertex(EliminationInformation info)}:
 Will revert the elimination of a vertex.
 \item\lstinline[language=Java]{getSimplicialVertex(Set<T> forbidden)}: Get an arbitrary simplicial vertex, that is not
   contained in the forbidden set. A simplicial vertex is a vertex that has a
   clique as neighborhood. As required information are already
   gathered during the construction of the graph, this method runs in
   $O(|V|)$.
 \item\lstinline[language=Java]{getAlmostSimplicialVertex(Set<T> forbidden)}: As the last method, but will return an \emph{almost}
   simplicial vertex, that is, a vertex that has a clique and one other
   vertex as neighborhood. This method actually computes the vertex
   and is more expensive than the last one.
 \item\lstinline[language=Java]{getFillInValue(T v)}: The fill-in
   value of an vertex $v$ is the amount of edges that will be
   introduced to the graph, if $v$ is eliminated. This method runs in
   $O(1)$ as well.
\end{enumerate}

\section{Reading and Writing Graphs}
In the previous section we have created a graph ``by hand'', in real
scenarios, however, we will often need to read the graph from
standard input or from a file. The class \JClass{GraphFactory}
provides a method to read \texttt{.gr} files (which is the graph format
specified by
PACE\footnote{\url{https://pacechallenge.wordpress.com/pace-2016/track-a-treewidth/}}. These
methods are quite generic, and also work for \texttt{DIMACS} (graph)
files, i.\,e., \texttt{.dgf} files. A typical usage would be:
\begin{lstlisting}[language=Java]
  Graph<Integer> myGraph = GraphFactory.graphFromStdin();
\end{lstlisting}
A graph, however, can not only be created from a stream. A common
source for a graph is, well, another graph. The \JClass{GraphFactory} class
provides methods to copy graphs:
\begin{lstlisting}[language=Java]
  Graph<Integer> myGraph = GraphFactory.copy(othergraph);
\end{lstlisting}
Another way to obtain a graph from a graph is by taking a subgraph,
that is, a graph defined by a subset of the vertices. In \Jdrasil,
this can be achieved by the following code:
\begin{lstlisting}[language=Java]
  Set<Integer> subgraph = new HashSet<>();
  // ... add vertices to subgraph ...
  Graph<Integer> myGraph = 
    GraphFactory.graphFromSubgraph(othergraph, subgraph);
\end{lstlisting}

Once \Jdrasil\ did its job, we most likely want to print the graph or
its tree decomposition to the standard output or a file. This can be
achieved with the class \JClass{GraphWriter}, which provides many
methods to print graphs. To simply write a graph, or a tree
decomposition, to the standard output, we can use the following code:
\begin{lstlisting}[language=Java]
  // write a graph of any type
  GraphWriter.writeGraph(myGraph);

  // write a graph of any type, translate vertices to {1,...,|V|}
  GraphWriter.writeValidGraph(myGraph);

  // write tree decomposition
  GraphWriter.writeTreeDecomposition(decomposition);
\end{lstlisting}
Additionally, \Jdrasil\ also provides methods to write graphs and tree
decomposition directly into Ti\emph{k}Z code, which is very useful for
debugging, especially combined with the \emph{graph drawing}
capabilities of Ti\emph{k}Z:
\begin{lstlisting}[language=Java]
  // write a graph to TikZ
  GraphWriter.writeTikz(myGraph);

  // write tree decomposition to TikZ
  GraphWriter.writeTreeDecompositionTikZ(decomposition);
\end{lstlisting}

\section{Storing Graphs on the Bit-Level}
\includeJavaDoc{../core/src/main/java/jdrasil/graph/BitSetGraph.java}

\chapter{Graph Invariants}
A graph property is a class of graphs that is closed under
isomorphisms. A graph invariant is a function that maps isomorphic
graphs to the same value. Examples for graph invariants are ``number of
vertices'', or ``size of the minimum vertex-cover''. An example of a
graph property could be ``contains a triangle''. In this sense, we can
model graph properties as invariants that map to boolean values.

\Jdrasil\ implements graph invariants over the interface \JClass{Invariant}. In order to do so, it
essentially provides the method \JMethod{getValue}, which returns the
computed invariant.  Since many invariants can be represented by an
additional model (for instance, a model for the vertex-cover model
is the actual vertex-cover), the class also provides the method
\JMethod{getModel}, which returns a map from vertices to some values.

The usual usage of an invariant is as follows (here, we use
vertex-cover):
\begin{lstlisting}[language=Java]
  // this will already compute the vertex cover
  VertexCover vc = new VertexCover<T>(myGraph);

  // the following methods then cost O(1)
  vc.getValue(); // size of the vertex-cover
  vc.getModel(); // the vertex-cover

  // since some invariants are hard to compute, we may not 
  // obtain an optimal solution, we can check as follows
  vc.isExact()
\end{lstlisting}

\section{Connected Components}
The class \JClass{ConnectedComponents} can be used to compute the
connected components of the graph. The model maps vertices to
integers, which represent the id of the connected component the vertex
is. In addition, the class provides methods to obtain the connected
components as sets of vertices and as subgraphs.

\section{Vertex Cover}
The class \JClass{VertexCover} computes a set of vertices that covers all edges. If an SAT solver is
available, this class will compute a minimal vertex-cover. Otherwise,
a 2-approximation is used.

\section{Clique}
The class \JClass{Clique} computes a set of vertices that are all pairwise adjacent. If an SAT
solver is available, a maximal clique will be computed. Otherwise, a
greedy strategy is used.

\section{Twin Decomposition}
The class \JClass{TwinDecomposition} computes a twin decomposition of
the graph. Two vertices $u$ and $v$ are
called twins if we have $N(u)=N(v)$. This relation defines an
equivalence relation on the graph, which we call twin decomposition.

\section{Minimal Separator}
The class \JClass{MinimalSeparator} computes a minimal separator, i.\,e., a minimal set of vertices such
that the removal of the set will increase the number of connected
components of the graph.

\section{Maximal Matching}
A matching in a graph \(G=(V,E)\) is a subset of the edges
\(M\subseteq E\) such that for every vertex \(v\in V\) we have
\(|\{\,e\mid e\in M\wedge v\in e\,\}|\leq 1\). A matching is \emph{maximal} if
we can not increase it by adding any edge. It is a \emph{maximum} matching,
if there is no bigger matching in the graph, and it is \emph{perfect} if
every vertex is matched.
  
In \Jdrasil, the class \JClass{Matching} greedily computes a maximal matching, i.\,e., it is not
guaranteed that it is a maximum matching. The matching is represented
as map from vertices to vertices, i.\,e., a vertex is mapped to the
vertex it is matched with.

\section{Cut Vertices}
A \emph{cut vertex} or \emph{articulation point} is a vertex whose removal
disconnects the graph. If the graph is biconnected, no such vertex
exists. The class \JClass{CutVertex} computes an arbitrary cut vertex using the
algorithm from Hopcroft and Tarjan in time $O(n+m)$.
 
The class allows to forbid some vertices in the graph, this can be used
to compute 2 or 3 connected components as follows: If the graph has not
cut vertex, but the graph without some vertex \(v\in V\) has a cut
vertex \(c\), then the pair \(v,c\) is a 2-cut.

\section{Clique Minimal Separator}
The class \JClass{CliqueMinimalSeparator} computes a clique minimal separator of the graph, that is, a set
\(S\subseteq V\) such that \(G[S]\) is a clique, \(G[V\setminus S]\)
has more components then $G$, and such that \(S\) is a minimal separator
for some vertices \(a,b\in V\).
 
This class takes \(O(nm)\) time and implements the algorithm described
in "An Introduction to Clique Minimal Separator Decomposition" by
Berry et al.\, In short, it does the following:
\begin{enumerate}
  \item compute a minimal triangulation of the graph;
  \item find minimal separators of the triangulation;
  \item check which of these separators are cliques in the graph.
 \end{enumerate}
 The class allows to specify a set of forbidden vertex, which are
 assumed to be not part of the graph then. In this way, the invariant
 can be used to compute almost clique minimal separators: take a
 vertex \(v\in V\) and remove it, if the resulting graph has a clique
 minimal separator \(S\), then \(S\cup\{v\}\) is an almost clique
 minimal separator.

\section{Minor-Safe Separator}
A separator \(S\) is safe for tree width, if for each connected
component \(C\) of \(G[V\setminus S]\) the tree width of the
graph \(G[C\cup S]\) with \(S\) as clique is bounded by the tree width
of \(G\).
 
Bodlaender and Koster have shown
that a separator \(S\) is safe in these terms if for each component
\(C\) in \(G[V\setminus S]\) the graph \(R=G[V\setminus C]\) contains
a clique on the vertices of \(S\) as labeled
minor~\cite{BodlaenderK2006}. We call such a separator \(S\)
\emph{minor-safe}.
 
The class \JClass{MinorSafeSeparator} implements a heuristic to find a minor-safe
separator. Whenever the heuristic outputs a separator, it will be
guaranteed that it actually is a minor-safe separator, however, if the
heuristic does not find such a separator there still could be one
contained in \(G\).

\section{Minimal Vertex Separator}
The class \JClass{MinimalVertexSeparator} implements an algorithm to compute a minimal vertex
separator between two sets \(S_A\), \(S_B\) in a given subgraph
\(G[W]\). The size of the separator can be bounded by a variable
\(k\), i.e., we will find the smallest separator less or equal to
\(k\) or report that no such separator exists.
 
In detail, this class implements a bounded version of the
Ford-Fulkerson algorithm running in time \(O(k(n+m))\).

\chapter{Tree Decompositions}\label{chapter:treedecompositions}
Since \Jdrasil{} is a library for computing tree decompositions, a
well thought through representation of such decompositions is
required. Recall the formal definition of a tree decomposition: A
\emph{tree decomposition} of a graph $G$ is a pair $(T,\iota)$
consists of a tree $T$ and a mapping $\iota$ from nodes of $T$ to
subsets of vertices of $G$ (called \emph{bags}), such that:
\begin{itemize}
  \item $\forall x\in V(G)$ there is a node $n\in V(T)$ with $x\in\iota(n)$;
  \item $\forall \{x,y\}\in E(G)$ there is a node $n\in V(T)$ with $\{x,y\}\subseteq\iota(n)$;
  \item $\forall x,y,z\in V(T)$ we have $\iota(y)\subseteq \iota(x)\cap\iota(z)$
whenever $y$ lies on the unique path between $x$ and $z$ in $T$.
\end{itemize}

\section{Design Principle}
The formal definition of tree decompositions and the way \Jdrasil\
represents graphs (remember that graphs can build up on any vertex
class) gives rise to the following approach: a tree decomposition in
\Jdrasil\ is pretty much just a \JClass{Graph} with a special vertex
class \JClass{Bag}. The \JClass{Bag} class on the other hand is actually just a
collection of vertices.

To put it all together, the class \JClass{TreeDecomposition}
represents a tree decomposition. It can store the original graph $G$,
and a graph representing the tree $T$ with \JClass{Bag}
vertices. Furthermore, the class \JClass{TreeDecomposition} provides
methods to create new nodes in the decomposition and to link existing nodes.

\section{Building and Using Tree Decompositions}
Let us assume we wish to implement a new algorithm for computing a
tree decomposition. The following code snippets illustrate how we would
create and store the decomposition in \Jdrasil. Note that during the
construction, the tree decomposition has not to be valid in any means.

Assume we read a graph with integer vertices from the standard
input. We can create a tree decomposition for the graph as
follows. At this point, the decomposition will be \emph{empty} and
\emph{invalid}, i.\,e., it will only correspond to the graph, but will
not have any information about the decomposition stored yet.
\begin{lstlisting}[language=Java]
  // read the graph G
  Graph<Integer> G = GraphFactory.graphFromStdin();
  // create empty tree decomposition for graph G
  TreeDecomposition<Integer> td = new TreeDecomposition<>(G);

  // check if the decomposition is valid
  boolean isValid = td.isValid(); // returns false
\end{lstlisting}
To create a bag, we need a subset of the vertices of $G$ that we wish
to store in the bag. Finding these subsets is of course the (hard) task of
the algorithm. Let us, in this example, just create the trivial tree
decomposition of a single bag:
\begin{lstlisting}[language=Java]
  Set<Integer> someVertices = G.getVertices();
  Bag<Integer> myBag = td.createBag(someVertices);

  // decomposition now is valid
  boolean isValid = td.isValid(); // returns true
\end{lstlisting}
The bag is created with the method \JMethod{createBag}. This will do
two things: it will create the \JClass{Bag} object and store it in the
tree decomposition, and it will return a reference to this
\JClass{Bag} object as well. This reference may be needed later on,
for instance, if we wish to connect some bags with a tree edge:
\begin{lstlisting}[language=Java]
  // anotherBag is a reference to another bag that we have created
  td.addTreeEdge(myBag, anotherBag);
\end{lstlisting}
And that is it. We have successfully created a tree
decomposition. \Jdrasil\ allows us to improve the quality of the
decomposition (whenever we know that we do not have an optimal
decomposition) with the separator technique
of~\cite{bodlaender2010treewidth}. Just do the following and the
decomposition may be improved:
\begin{lstlisting}[language=Java]
  td.improveDecomposition();
\end{lstlisting}
This method, however, may take some time if there are huge bags and
should therefore be used on already good decompositions (and not on the
trivial one as in this example).

\part{Algorithms}
At its core engine, \Jdrasil\ implements a bunch of algorithms to
compute tree decompositions. These algorithms either directly compute
such decompositions, or assist other algorithms in doing so. In
particular, the implemented algorithms (that are related to the
computation of tree decompositions) are partitioned into five types:
\emph{Preprocessing} algorithms, algorithms that compute
\emph{lowerbounds}, \emph{heuristics}/\emph{upperbounds}, \emph{approximation}
algorithms, and \emph{exact} algorithms.

\chapter{Preprocessing}
A Preprocessor is a function that maps an arbitrary input graph to an
``easier'' graph.  Easier here is meant with respect to computing a tree
decomposition and often just means ``smaller'', but could also reefer to
adding structures to the graph that improve pruning potential.
 
The class \JClass{Preprocessor} models a preprocessor by providing the methods
\JMethod{preprocessGraph,} \JMethod{addbackTreeDecomposition}, \JMethod{computeTreeDecomposition},
\JMethod{getTreeDecomposition}. The first method represents the
actual preprocessing and computes a smaller graph from the input
graph.  The following two methods can be used to add a tree
decomposition of the reduced graph produced by the first method back,
and to use this tree decomposition to create one for the input
graph. The last method is a getter for this decomposition.

The usual way to use a preprocessing algorithm is by a) initializing an
instance of it, b) get the generated graph, and c) add back a
tree decomposition for this graph. For instance, if we wish to
apply standard reduction rules, we could do the following:
\begin{lstlisting}[language=Java]
  
  // a) generate instance of preprocessing algorithm
  GraphReducer<T> reducer = new GraphReducer<>(G);

  // b) get the generated graph
  Graph<T> H = reducer.getProcessedGraph();

  // c) add the decomposition of H
  TreeDecomposition<T> td = ...
  reducer.addbackTreeDecomposition(td);

  // we can now access the final decomposition of the original graph
  reducer.getTreeDecomposition();
\end{lstlisting}

\section{Reduction Rules}
There are many reduction rules for tree width known in the
literature, see for instance~\cite{DowneyF2013}. A reduction rule thereby is a function that removes (in
polynomial time) a vertex from the graph and creates a bag of the
tree decomposition such that this bag can be glued to an optimal
tree decomposition of the remaining graph~–~yielding  an optimal
tree decomposition. For graphs of tree width at most 3, these rules
produce an optimal decomposition in polynomial time.  For graphs with
higher tree width, the rules can only be applied up to a certain
point. From this point on, another algorithm has to be used.

In \Jdrasil, the class \JClass{GraphReducer} implements several
reduction rules and can be used as preprocessing for any other
algorithm. As mentioned before, this class will (automatically)
compute optimal tree decompositions of graphs with tree width at most
$3$. If this class fully reduces the input graph, it will generate an
empty graph.

\section{Contracting the Graph}
It is a well known fact that tree width is closed under taking
minors\footnote{A \emph{minor} of $G$ can be formed by deleting edges
  and vertices and contracting edges.},
i.\,e., if \(H\) is a minor of \(G\), then \(\mathrm{tw}(H)\leq
\mathrm{tw}(G)\). Many algorithms that compute tree decompositions use
this fact in some way.
  
The \JClass{GraphContractor} class computes a matching of the input graph, and
contracts it. The result is a minor (which is of course much smaller)
that is returned. Given a tree decomposition of this minor, a tree
decomposition for the original graph is generated by decontracting the
edges within the bags of the decomposition. The result is a valid (but
not optimal) tree decomposition of the input graph. Furthermore, if
the decomposition of the minor has width \(k\), the width of the final
decomposition is at most \(2k+1\).
  
\section{Computing Improved Graphs}
The $k$-$\{\text{neighbor, path}\}$-improved graph of $G$ is the graph
that is obtained by repeatedly adding edges between non-adjacent
vertex $u,v$ that have at least $k$ $\{$common neighbors, vertex
disjoint paths between them$\}$. Note that this operation is not
necessarily safe. We have, however, \(\mathrm{tw}(G)\leq
k\Leftrightarrow\mathrm{tw}(H)\leq k\) where \(H\) is the
\((k+1)\)-improved graph of \(G\).~\cite{bodleanderK2011}.

The improved graphs are useful, as adding edges improves the quality
of lower bound algorithms and may also boost the pruning potential of
exact algorithms. In \Jdrasil{} they can be computed by the classes
\JClass{NeighborImprovedGraph} and \JClass{PathImprovedGraph}, respectively. 

\chapter{Splitting up the Graph}
When we compute the tree width of a graph, it is often not necessary
to compute the tree width of the whole graph at once. In stead, it is
often possible to \emph{split} the graph into some components that can
be handled separately. For instance, each connected component can be
decomposed for its one.

A separator $S\subseteq V$ of a graph $G=(V,E)$ is a subset of the
vertices such that $G\setminus S$ has more connected components
than $G$. In particular, the connected components of $G$ are separated
by the separator $\emptyset$, while biconnected components have a
separator $S$ with $|S|=1$.

Bodlaender and Koster have presented a list of \emph{safe} separators
for tree width~\cite{BodlaenderK2006}. A safe separator is one that
does not effect the tree width, i.\,e., one that allows to reproduce
an optimal tree decomposition for $G$ from optimal tree decompositions
of the connected components of $G\setminus S$ (in polynomial
time). Having such safe separators allows us to compute tree
decomposition in an divide-and-conquer manner: Split the graph using
safe separators until the graph is small enough to solve it, or until
there are no separators left. We call these unsplittable graphs \emph{atoms}.

In \Jdrasil, the class \JClass{GraphSplitter} implements some safe
separators. The class will split the graph using such separators, and
will keep track of potential glue points. Once the class is provided
with tree decompositions for all components produced, it will generate
a tree decomposition for the original graph.

In particular, \JClass{GraphSplitter} splits the graph using the
following separators:
\begin{enumerate}
  \item separators of size $0$, i.\,e., it splits into connected
    components;
  \item separators of size $1$, i.\,e., it splits into biconnected
    components;
  \item separators of size $2$, i.\,e., it splits into triconnected
    components;
  \item safe separators of size $3$;
  \item clique minimal separators;
  \item almost clique minimal separators;
  \item minor-safe separators found by a heuristic.
\end{enumerate}
The \JClass{GraphSplitter} makes use of two Java concepts in order to
make it easy to use and efficient.

\emph{Splitting the graph using recursive tasks.} First of all, it implements its
divide-and-conquer procedure using Javas \JClass{RecursiveTask}
interface. This let Java handle the recursion efficiently and, if the
``parallel'' flag is set, leads to an direct and automatic parallelization (each atom can
be handled in parallel).

\emph{Handling atoms using Lambda expressions.} Splitting the graph
into safe components is a generic task that may be used by many
different tree decomposition algorithms (let it be exact, approximation,
or heuristic algorithms). Hence, the \JClass{GraphSplitter} is
designed to handle atoms with an arbitrary function that it obtains as
parameter. In particular, the \JClass{GraphSplitter} expects a
functions as parameter that maps graphs to tree decompositions, and
this function will be applied to all atoms. Afterwards, the created
tree decompositions will be glued together. All of this will happen in
parallel, if the ``parallel'' flag is set.

The usual way of using the \JClass{GraphSplitter} is straight forward:
\begin{lstlisting}[language=Java]
  // 1. create a splitter for the graph G
  GraphSplitter<T> splitter = new GraphSplitter<T>(G, H -> {
    // 2. create a tree decomposition of the atom H
                TreeDecomposition<T> td = ...
                return td;
            },lb);

  // 3. Obtain a tree decomposition of G
  // this will invoke the splitting process
  TreeDecomposition<T> final = splitter.call();
\end{lstlisting}
Note that, intuitive, one would first split the graph and then apply
preprocessing to the atoms. However, splitting the graph is more
expensive then preprocessing, so the advice is to apply preprocessing
first and to split the result afterwards. However, it can also be
useful to apply preprocessing to the atoms again.

\chapter{Lowerbounds}
The \JClass{lowerbound} interface describes a class that models an
algorithm for computing a lower bound of the tree width of a graph.
By implementing this interface, a class has to implement the Callable
interface, which computes a lower bound on the tree width 
a graph. In addition, a class implementing this interface has to
implement the method \JMethod{getCurrentSolution}, which can be used
if the lower bound algorithm can already provide lower bounds during
its execution. \Jdrasil{} implements the following lowerbound algorithms.

\section{Degeneracy of a Graph}
We call a Graph $G=(V,E)$ $d$-degenerated if each subgraph $H$ of $G$ contains
a vertex of maximal degree $d$.  It is a well known fact that we have
\(d \le tw(G)\) and, thus, we can use the degeneracy of a graph as
lowerbound for the tree width.

The class \JClass{DegeneracyLowerbound} implements the linear time
algorithm from Matula and Beck~\cite{MatulaB1983} to compute the
degeneracy of a graph.

\section{Lowerbounds based on Minors}
It is a well known fact that for every minor $H$ of $G$ the following
holds: \(tw(H) \le tw(G)\). To obtain a lowerbound on the tree width
of $G$ it is thus sufficient to find good lowerbounds for minors of
$G$.  The minor-min-width heuristic devoloped by Gogate and Dechter
for the QuickBB algorithm~\cite{GogateD2004} does exactly this. It
computes a lowerbound for a minor of G and tries heuristically to find
a good minor for this task.

In \Jdrasil{} the class \JClass{MinorMinWidthLowerbound} implements
this heuristic with different strategies discussed by Bodlaender and Koster~\cite{bodleanderK2011}.

\section{Compute Lowerbound in Improved Graphs}
The \{$k$-neighbor, $k$-path\}-improved graph \(H\) of a given graph \(G\)
is obtained by adding an edge between all non adjacent vertices that
have at least $k$ \{common neighbors, vertex disjoint paths\}. A crucial
lemma states that adding these edges will not increase the tree
width. Hence, we can compute a lower bound on the tree width of \(G\)
by computing lower bounds on increasing improved graphs of \(G\). In
this way, improved graphs can be used to improve the performance of
any lower bound algorithm.

The class \JClass{ImprovedGraphLowerbound} implements the improved graph trick to improve some
implemented lower bound algorithms. The implementation is based on~\cite{bodleanderK2011}.

\chapter{Heuristics}
Since computing the exact tree width of a graph is
$\Class{NP}$-hard, there are many graphs for which we are not yet
able to compute an optimal tree decomposition. However, it turns out
that there are very powerful heuristics for this problem.

In \Jdrasil{}, all heuristics implement the interface
\JClass{TreeDecomposer} and produce tree decompositions of quality ``Heuristic''.

In addition to the different implemented algorithms, there is the
class \JClass{Heuristic} which provides stand alone access to a
combination of heuristics and is used by us for the PACE heuristic
track. This class can be compiled and used with
\begin{lstlisting}[language=bash]
  ./gradlew heuristic
  ./tw-heuristic
\end{lstlisting}
The program combines some of the implemented heuristics to get the
best from each world. In particular, it does the following:
\begin{enumerate}
  \item reduce the graph using \JClass{GraphReducer};
  \item computes a first decomposition using;
    \JClass{StochasticGreedyPermutationDecomposer};
  \item tries to improve the decomposition;
  \item starts an infinite local search to further improve the
    decomposition using the algorithm implemented in \JClass{LocalSearchDecomposer}.
\end{enumerate}
The program will try to improve the decomposition, until a
\texttt{SIGTERM} is received. 

\section{Greedily Compute an Elimination Order}
The class \JClass{GreedyPermutationDecomposer} implements greedy
permutation heuristics to compute a tree decomposition. The heuristic
eliminates the vertex $v$ that minimizes some function $\gamma(v)$,
while ties are broken randomly. See~\cite{bodlaender2010treewidth}
for an overview of possible functions $\gamma$. The class implements
six different value functions, which can be selected by the method
\JClass{setToRun}. Experiments have shown that different functions are
preferable on different graphs. It is, thus, worth to test different
value function when we compute a tree decomposition of a graph.

To improve the quality of the found decomposition, we can do some sort
of \emph{look-ahead}: instead of taking the vertex that minimizes
$\gamma$, we take the vertex such that the sum of the next $k$ choices
minimizes $\gamma$. A look-ahead for $k>1$ can be set with
\JClass{setLookAhead}. Already for $k=2$ this improves the quality of
the heuristic on many graphs. However, increasing $k$ by one increases
the running time by a factor of $|V|$ and, hence, the look-ahead
should be used with care.

\section{Greedily Compute an Elimination Order with Many Coins}
The Greedy-Permutation heuristic performs very well and can be seen as
\emph{randomized algorithm} as it breaks ties randomly.  Therefore,
multiple runs of the algorithm produce different results and, hence,
we can perform a stochastic search by using the heuristic multiple
times and by reporting the best result. As the
Greedy-Permutation heuristic implements different algorithms, we can
pick different algorithms in different runs. As the performance of
these algortihms differ, we choose them with different probabilities.
In \Jdrasil{}, this strategy is implemented by the class \JClass{StochasticGreedyPermutationDecomposer}.

\section{Maximum Cardinality Search}
The class \JClass{MaximumCardinalitySearchDecomposer} implements the
Maximum-Cardinality Search heuristic. The heuristic orders the vertices
of $G$ from $1$ to $n$ in the following order: We first put a
random vertex $v$ at position $|V|$. Then we choose the vertex $v'$
with the most neighbors that are already placed at position $|V|-1$
and recurse this way. Ties are broken randomly.

\section{Local Search}
The class \JClass{LocalSearchDecomposer} implements a tabu search on
the space of elimination orders developed by Clautiaux, Moukrim,
N{\`e}gre, and Carlier~\cite{clautiaux2004heuristic}.

The algorithm expects two error parameters $r$ and $s$: the number of
restarts and the number of steps.  To find a tree decomposition the
algorithm starts upon a given permutation, then it will try to move a
single vertex to improve the decomposition and do this $s$ times. If
no vertex can be moved, a random vertex is moved and the process
restarts. At most $r$ restarts will be performed.  At the end, the
best found tree decomposition is returned.

\section{Greedy Path Decompositions}
A path decomposition can be seen as a special case of a tree
decomposition that is sometimes easier to handle. In particular
heuristics for path decompositions can be simpler and more flexible
then the ones for tree decompositions. In \Jdrasil{} one such
heuristic is provided by the class \JClass{GreedyPathDecomposer},
which greedily computes a path decomposition by adding vertices to the
current bag such that other vertices can be removed quickly from the bag.

\chapter{Approximation}
An approximation algorithm as the one by Robertson and Seymour~\cite{RobertsonS1995} can be interesting in two ways: it
produces lower \emph{and} upper bounds at the same time, while also
provides a guarantee on its quality. This section lists the
approximation algorithms implemented by \Jdrasil.

In \Jdrasil{}, all approximation algorithms implement the interface
\JClass{TreeDecomposer} and produce tree decompositions of quality ``Approximation''.

In addition to the different implemented algorithms, there is the
class \JClass{Approximation} which provides stand alone access to a
combination of approximation algorithms.
\begin{lstlisting}[language=bash]
  ./gradlew approximation
  ./tw-approximation
\end{lstlisting}
The program combines some of the implemented approximations to get the
best from each world. In particular, it does the following:
\begin{enumerate}
  \item split the graph into safe components using \JClass{GraphSplitter}
  \item reduce the graph using \JClass{GraphReducer};
  \item computes a decomposition using \JClass{RobertsonSeymourDecomposer}.
\end{enumerate}

\section{Robertson and Seymour like Approximation}
The class \JClass{RobertsonSeymourDecomposer} uses the standard
$\Class{FPT}$\footnote{$\Class{FPT}$ stands for \emph{fixed-parameter
    tractable}.} approximation algorithm for tree width based on the work of Robertson and Seymour.
The algorithm assumes that the graph is connected and computes in time \(O(8^k k^2 \cdot n^2)\) a tree decomposition
of width at most \(4k+4\).

A detailed explanation of the algorithms can be found in many text
books about $\Class{FPT}$, for instance~\cite{CyganFKLMPPS2015, FlumGrohe2006}.

\chapter{Exact}
At the heart of \Jdrasil{} is a collection of algorithms that compute
optimal tree decompositions. All these algorithms implement the interface
\JClass{TreeDecomposer} and produce tree decompositions of quality ``Exact''.

In addition to the different implemented algorithms, there is the
class \JClass{Exact} which provides direct access to some
decomposition algorithms. This class can be compiled and used with
\begin{lstlisting}[language=bash]
  ./gradlew exact
  ./tw-exact
\end{lstlisting}
As different algorithms may be preferable on different graphs, the
program selects between different algorithms, in particular it will:
\begin{enumerate}
  \item split the graph into safe components using \JClass{GraphSplitter};
  \item reduce the graph using \JClass{GraphReducer};
  \item solve it with \JClass{CatchAndGlue}
\end{enumerate}

\section{Branch and Bound}
The class \JClass{BranchAndBoundDecomposer} implements a classical branch and bound algorithm based on QuickBB~\cite{GogateD2004} and its successors.
The algorithm searches through the space of elimination orders and utilizes dynamic programming,
reducing the search space to $O(2^n)$.

\section{Cops and Robber}
A classic result of Seymour and Thomas~\cite{SeymourT1993}
provides a connection of the tree width of the graph and the
cops-and-robber search game. Together with an algorithm by Berarducci
and Intrigila~\cite{BerarducciI1993} to evaluate such games in time
$n^{O(tw(G))}$, this yields a way to compute exact tree decompositions. In
\Jdrasil{} the class \JClass{CopsAndRobber} implements this approach.

\section{Cops and Robber (Bottom-Up)}
Tree width has nice game theoretic characterisations, one of which is
the node-search game played by a set of searchers and a fugitive. The
variant of the game that corresponds to tree width is the one with a
visible fugitive of unbounded speed (also known as the helicopter cops
and robber game, defined by Robertson and Seymour).
 
In \Jdrasil, the class \JClass{CatchAndGlue} computes a winning strategy for the cops (from which a tree
decomposition can be deduced) in a bottom-up fashion.  In order to do
so, a queue of win-configurations is maintained which is pre-filled
with trivial win-configurations, i.\,e., configurations in which the
robber will be caught immediately. Then predecessor configurations
(with respect to a winning strategy of the cops) are computed for the
configurations in the queue, and if these configurations are
win-configurations as well, they are added back to the queue.  As larger
win-configurations are "better" in the sense that we wish to find a
win-configuration for the whole graph, a priority queue is used that
orders configurations by size.
 
Computing predecessor configurations is straighted forward if the cops
move does not disconnect the graph (i.\,e. a simple existential
"fly"-move), however, it is difficult if a universal "split"-move is
reconstructed (i.e., if the current configuration of the game
originated from a cops move that has disconnected the graph and from a
corresponding robber move whom has chosen a connected component). In
this scenario multiple win-configurations (which we may or may not
already have discovered) have to be glued together. These
configurations correspond to branch nodes in the tree decomposition.

The asymmetrical running time of this algorithm is worse then the one
of the \JClass{CopsAndRobber} class, as the ``glue''-part may cost
exponential time as well. However, in practice there are much less
configurations considered over all if we compute the winning strategy
backwards, as we can discard a lot of ``lose''-configurations and
sinks in the configuration-graph.

\section{Positive-Driven BT}
\includeJavaDoc{../core/src/main/java/jdrasil/algorithms/exact/PidBT.java}

\section{Limited Graph Search}
The tree width of a graph can be characterized from a game theoretic
point of view with the graph search game where a team of searchers
tries to catch a fugitive, who has unlimited speed. Depending on the
visibility of the fugitive, a winning strategy of the searchers
corresponds to a path- or tree-decomposition of the graph. Fomin,
Fraigniaud and Nisse described a mixed version of the game~\cite{FominFN09}.
Here, the fugitive is invisible, but he searchers may reveal her from
time to time. By fixing the number \(q\) of reveals that are performed
by the searchers, the game covers path decompositions (\(q=0\)) and
tree decompositions (\(q=\infty\)), and, in particular, the area
between these two extrema. In the cited paper such decompositions are
called q-branched tree decompositions and they have, loosely speaking,
the property that on each path from a root to the leafs there are at
most \(q\) nodes with more then one children, i.e., at most \(q\)
branching nodes.

In \Jdrasil{} the class \JClass{LimitedGraphSearch} implements a
dynamic program that computes a winning strategy of the searchers in
the limited search game and, thus, computes a $q$-branched tree
decomposition. The class will compute the smallest $k$ such that there
is a winning strategy for a fixed $q$, or alternatively the smallest
$k$ such that any $q$ exists such that the searcher win with $q$ reveals.

\section{Dynamic Programming}
The class \JClass{DynamicProgrammingDecomposer} implements exact exponential time (and exponential space)
algorithms to compute a tree decomposition via dynamic programming.
The algorithms are based on the work of 
Bodlaender, Fomin, Koster, Kratsch, and Thilikos~\cite{BodlaenderFKKT2012}.

\section{Naive Brute Force}
The tree width characterization via elimination order gives a very
simple brute force algorithm: just check all $n!$ permutations. This
is of course only feasible for very small graphs. The approach is
implemented by the class \JClass{BruteForceEliminationOrderDecomposer}.

\section{Using a SAT Solver}
A very common (theoretical and practical) approach to solve intractable
problems is to first represent them as \emph{constraint satisfaction
  problems} (CSP) and then solve those problems via specialized solvers. The
most widely used solvers are SAT solvers that work on Boolean
formulas.

\Jdrasil{} provides the classes \JClass{BaseEncoder} and
\JClass{ImprovedEncoder} to encode the problem of finding an optimal
tree decomposition into a logic formula. The class
\JClass{SATDecomposer} constructs such formulas, solves them using a
SAT solver, and extracts the corresponding tree decomposition.

The class \JClass{SATDecomposer} only works if a SAT solver is
installed as upgrade, see Section~\ref{upgrade:sat} for details.

\chapter{Postprocessing}
\includeJavaDoc{../core/src/main/java/jdrasil/algorithms/postprocessing/Postprocessor.java}
\section{Improving Tree Decompositions}
\includeJavaDoc{../core/src/main/java/jdrasil/algorithms/postprocessing/ImproveTreeDecomposition.java}
\section{Flatten Tree Decompositions}
\includeJavaDoc{../core/src/main/java/jdrasil/algorithms/postprocessing/FlattenTreeDecomposition.java}
\section{Nice Tree Decompositions}
\includeJavaDoc{../core/src/main/java/jdrasil/algorithms/postprocessing/NiceTreeDecomposition.java}

\chapter{Dynamic Programming on Tree Decompositions}
\includeJavaDoc{../core/src/main/java/jdrasil/workontd/DynamicProgrammingOnTreeDecomposition.java}
\section{State Vectors}
\includeJavaDoc{../core/src/main/java/jdrasil/workontd/StateVector.java}
\section{State Vector Factory}
\includeJavaDoc{../core/src/main/java/jdrasil/workontd/StateVectorFactory.java}

\section{Example: Graph Coloring}
To illustrate the usage of dynamic programming over tree
decompositions in \Jdrasil{}, we will turn to the most simple example:
graph coloring. More precisely, we wish to implement an algorithm that
determines if a given graph can be vertex-colored with $3$ colors such
that no two adjacent vertices get the same color. To get started, we
have to implement a class that implements the \JClass{StateVector}
interface. This class should represent the states of the dynamic
program at a single node of the tree decomposition. A state in the
graph coloring problem is a specific coloring of the vertices of the
bag, so \JClass{ColoringStateVector} should represent a collection of
possible colorings of the vertices in the bag represented by the node.

To represent a single coloring, we will introduce a simple wrapper
class \JClass{State}, which essentially manages an array that assigns
colors to vertices. Observe that the array has only size bounded by
the tree width, as we may use the tree-index to index it.
\begin{lstlisting}[language=Java]
class State {
  int[] colors;

  public State(int tw) {
    this.colors = new int[tw+1];
  }

  public State(State o) {
    this.colors = Arrays.copyOf(o.colors, o.colors.length);
  }

  @Override
  public boolean equals(Object o) {
    if (this == o) return true;
    if (o == null || getClass() != o.getClass()) return false;
    State that = (State) o;
    return Arrays.equals(colors, that.colors);
  }

  @Override
  public int hashCode() {
    return Arrays.hashCode(colors);
  }
}
\end{lstlisting}

Given the wrapper class, our \JClass{ColoringStateVector} essentially
boils down to mange a set of states, i.\,e., \JMethod{Set<State>
  states}. We now have to implement the behaviour for introducing or
forgetting vertices, joining bags, and introducing edges. We handle
\emph{introduce vertex} first. This is quite easy, as we just have to
assign the new vertex one of the three possible colors. This means
that we have to create three copies of every old state and assign the
new vertex a different color in each of these copies. Observe that we
use the given tree-index to assign a color in the state. Also observe that we
assign the colors $1$, $2$, and $3$~--~we use ``color'' $0$ to
indicate that a vertex with this index is not in the bag.

\note{For simplicity, we do not care about adjacent vertices getting the same color
  here, as we will use edge-bags for this purpose. Of course, we could
also perform this test directly here and do not use edge-bags at all.}
\begin{lstlisting}[language=Java]
@Override
public StateVector<Integer> introduce(Bag<Integer> bag, Integer v,
    Map<Integer, Integer> treeIndex) {
  Set<State> newStates = new HashSet<>();
  for (State state : states) {
    for (int color = 1; color <= 3; color++) {
      State newState = new State(state);
      newState.colors[treeIndex.get(v)] = color;
      newStates.add(newState);
    }
  }
  this.states = newStates;
  return this;
}
\end{lstlisting}

The next thing on our checklist are forget-bags in which we have to
\emph{forget a vertex}. This essentially means that we set the entry
of its tree-index to $0$ (meaning that there is no vertex with this
index in the bag). The crucial point is that this may result in
multiple states being equivalent (for instance, assume we have a bag
with four vertices and the states $(1,2,3,3)$ and $(1,2,3,4)$~--~if we
delete the last vertex both states are the same). In order to
guarantee the usual run time of a dynamic program over tree
decompositions, we are not allowed to carry duplicates
around. Therefore, we have to detect all duplicates created by the
forget bag and remove them from the state set. Fortunately for us,
this property can automatically be enforced by the underling hash set.

\begin{lstlisting}[language=Java]
@Override
public StateVector<Integer> forget(Bag<Integer> bag, Integer v,
    Map<Integer, Integer> treeIndex) {
  Set<State> newStates = new HashSet<>();
  for (State state : states) {
    state.colors[treeIndex.get(v)] = 0;
    newStates.add(state);
  }
  this.states = newStates;
  return this;
}
\end{lstlisting}

The third bag type we have to handle are join-bags, which connect two
children with the same content. Recall that a state is a coloring of
the current bag that is valid for the whole subtree rooted at this
bag. Hence, valid states at a join-bag are exactly the once's that
appear in both children. This means that all we have to do at a join-bag is to simply compute the intersection of the two state vectors.

\begin{lstlisting}[language=Java]
@Override
public StateVector<Integer> join(Bag<Integer> bag,
StateVector<Integer> stateVector,
    Map<Integer, Integer> treeIndex) {
  ColoringStateVector o = (ColoringStateVector) stateVector;

  Set<State> newStates = new HashSet<>();
  if (this.states.size() < o.states.size()) {
    for (State state : this.states) {
      if (o.states.contains(state)) newStates.add(state);
    }
  } else {
    for (State state : o.states) {
      if (this.states.contains(state)) newStates.add(state);
    }
  }

  this.states = newStates;
  return this;
}
\end{lstlisting}

The last thing on our checklist are edge-bags, which \emph{introduce
  an edge} to the bag. Recall that every edge is introduced exactly
once. Since we do not check if the coloring is valid when we introduce
a new vertex, we have to do this now, i.\,e., given the introduced
edge we have to verify that the two connected vertices have different
colors. States in which this property is not satisfied have to be
removed.

\begin{lstlisting}[language=Java]
@Override
public StateVector<Integer> edge(Bag<Integer> bag, Integer v, Integer w,
    Map<Integer, Integer> treeIndex) {
  Set<State> newStates = new HashSet<>();
  for (State state : states) {
    if (state.colors[treeIndex.get(v)] != treeIndex.colors[map.get(w)])
      newStates.add(state);
  }
  this.states = newStates;
  return this;
}
\end{lstlisting}

We are actually almost done. The class \JClass{ColoringStateVector}
does everything it has to. The only thing missing is a way to tell
\Jdrasil{} to use this state vector class. This is done by providing a
class that implements the \JClass{StateVectorFactory} interface and
that is able to produce state vectors for the leafs. Usually these
state vectors are just empty~--~this is true for the coloring
problem. We can therefore use a quite simple implementation for
\JClass{StateVectorFactory}.

\note{If we wish to compute the chromatic number of the graph (i.\,e.,
the smallest number of colors needed), a convenient way is to give the
coloring state vector a parameter $q$ of allowed colors and to
provide different factories to the solver.}
\begin{lstlisting}[language=Java]
public class ColoringStateVectorFactory
    implements StateVectorFactory<Integer> {

  @Override
  public StateVector<Integer> createStateVectorForLeaf(int tw) {
    return new ColoringStateVector(tw);
  }
  
}
\end{lstlisting}

We are now finally ready to run the algorithm. Essentially, all we have to do
is to provide
the \JClass{DynamicProgrammingOnTreeDecomposition} class with the
input graph and an instance of our factory. If we run the program the
``magic'' happens in the background, i.\,e., a tree decomposition is
computed, optimized, and traversed. The \JClass{ColoringStateVector}
class is internally used to transfer information from the leafs to the
root and, finally, we obtain the state vector assigned to the root. If
this vector is not empty, the graph is 3 colorable (as the valid
colorings of the bag correspond to valid colorings of the whole
subtree). On the other hand, if the vector is empty the graph can not
be colored with 3 colors.

\note{If we would provide ``false'' to the solver it would not compute
edge-bags. Our algorithm would then not work. However, if we would check the
coloring when we introduce a vertex we would obtain a correct algorithm again.}
\begin{lstlisting}[language=Java]
Graph<Integer> G = GraphFactory.emptyGraph();
// fill G with custom content

DynamicProgrammingOnTreeDecomposition<Integer> solver
  = new DynamicProgrammingOnTreeDecomposition<Integer>(
    G, new ColoringStateVectorFactory(), true
  );
ColoringStateVector rootVector = (ColoringStateVector) solver.run();

if (rootVector.states.size() >= 0) {
  System.out.println("3 colorable");
} else {
  System.out.println("not 3 colorable");
}
\end{lstlisting}

\part{Upgrades}\label{part:upgrades}

\Jdrasil\ is designed as a modular and platform independent library,
which provides all the advantages discussed earlier, but also comes
with a couple of problems. In particular, in order to compute a tree
decomposition of a graph, \Jdrasil\ internally solves many different
combinatorial optimization problems. Some of these problems may be
solved more efficiently on a specific target platform, rather than on
Javas virtual machine. For instance, one may want to use present
graphic cards for massive parallelization. On the other hand, for many
of these problems there are excellent and optimized libraries
available, which we want to use. For instance, \Jdrasil\ will rather
use existing \Lang{SAT} solvers to solve the boolean satisfiability
problem, instead of implementing its own. The concept of
\emph{upgrades} is \Jdrasil's way to use external code or
libraries. 

We use the term ``upgrade'', in contrast to something like
``library'', as \Jdrasil\ will always be fully functional and platform
independent without any upgrade. In particular, it can be compiled and
shipped without any upgrade. On the other hand, an upgrade will, as
the name suggests, speed up \Jdrasil\ on certain instances or
platforms. In other words, an upgrade will not increase the
functionality of \Jdrasil, but will provide tools for \Jdrasil\
such that it can execute its functionality faster.

The default location of upgrades for \Jdrasil\ is the folder
\file{build/upgrades/}. Since \Jdrasil\ comes
without any upgrade, this folder, at default, does not contain
much. However, the gradle file of \Jdrasil\ provides some targets
to obtain upgrades.
\begin{center}
\vfill
\fbox{
  \begin{minipage}{0.75\textwidth}
  \vspace{1ex}
  ~\hfill\textcolor{jdrasil.alert}{Licence Warning}\hfill~
  \vspace{0.5ex}

  While \Jdrasil\ stands under the open MIT licence, the third party
  software installed through upgrades may not. Whenever we install an
  upgrade, we may have to restrict the licence. Please read the
  licence of used third party software before you install an upgrade.
  \end{minipage}
}
\vfill
\end{center}
\chapter{Boolean Satisfiability}\label{upgrade:sat}
The boolean satisfiablity problem \Lang{SAT} is the most canonical
$\Class{NP}$-complete problem, and ``simply'' asks if a boolean
formula in CNF has a satisfying model. Many $\Class{NP}$-complete
problems can naturally be stated as a \Lang{SAT}-problem, and hence, can
be naturally solved by finding a model for a CNF formula. This is the
reason why \emph{SAT solvers}, i.\,e., tools that solve the boolean
satisfiablity problem, have received a lot of research effort. In
particular, there are annual challenges\footnote{\url{http://baldur.iti.kit.edu/sat-competition-2016/}} that try to
find the fastest solver. The result of this effort is that modern
SAT solver can solve hard problems on many instances very quickly.

The power of SAT solvers makes it interesting to use them while
computing a tree decomposition. Equipped with a SAT solver, \Jdrasil\
can directly encode the problem of finding a tree decomposition (or
more precisely, an elimination order) into a $\Lang{SAT}$-formula. On the other hand, \Jdrasil\ can
also use the SAT solver to solve different subproblems while computing
the tree decomposition. For instance, if \Jdrasil\ computes an
elimination order, it can always put a clique of the graph at the
end of the permutation. If the clique is large, this can reduce the
search space dramatically. However, finding large cliques in a graph is
$\Class{NP}$-hard as well and \Jdrasil\ will thus use a SAT solver to find the
largest clique in the graph.

\section{The Formula Class}\label{section:satFormula}
The main interface of \Jdrasil\ to use boolean logic is the class
\JClass{Formula}, which represents a boolean formula in
CNF. This class is always available and can always be used to create
and manage logic formulas. 

\subsection{Specifying a Formula}
A formula $\phi$ is always represented in CNF and in the classic
DIMACS format, that is, variables are positive integers
$x\in\mathbb{N}$, and negated variables are simply stored as $-x$. We
can specify a formula by adding clauses to it, for instances
$\phi=(x_1\vee\neg x_2\vee x_3)\wedge(x_2\vee\neg x_3)\wedge x_3$ can
be created as follows:

\begin{lstlisting}[language=Java]
  Formula phi = new Formula();
  phi.addClause(1, -2, -3);
  phi.addClause(2, -3);
  phi.addClause(3);
\end{lstlisting}

We can also ``concatenate'' two formulas by combining them with a logic
``and'', i.\,e., we can compute $\phi\wedge\psi$:

\begin{lstlisting}[language=Java]
  Formula psi = new Formula();
  psi.addClause(-1);
  
  phi.and(psi);
\end{lstlisting}

We can always add clauses to an existing formula or concatenate it
with another formula. With other words, we can always further
restrict the solution space of a formula. Sometimes, however, we may
wish to remove a clause, which can be done by:
\begin{lstlisting}[language=Java]
  psi.removeClause(-1);
\end{lstlisting}
But this operation should be used with caution: first of all it is
much more expensive to remove a clause than adding one; and,
furthermore, we are not always allowed to remove a clause (the method
can throw an exception). The reason for this is that SAT solvers that
solve a formula incrementally often only allow to restrict the formula
further. This means, once we started to ``solve'' the formula, we
can not remove clauses anymore.

\subsection{Solving a Formula}
So, \note{We can only solve formulas if a SAT solver is installed as upgrade.}
how to actually find a model for the formula, i.\,e., how to
``solve'' it. In order to check if a formula has a satisfying
assignment, \Jdrasil\ uses external SAT solvers which have to be
installed as upgrade (see the following sections for possible
solvers). If a SAT solver is installed, we can register it to the
formula:
\begin{lstlisting}[language=Java]
  String sig = phi.registerSATSolver();
\end{lstlisting}
This method will register an arbitrary SAT solver that \Jdrasil\ has
found as upgrade. If no SAT solver is installed, this method will
throw an exception; otherwise the signature of the solver is
returned. Once a solver is registered, the following will happen:
\begin{enumerate}
  \item The formula ``as is'' will be transfered to the solver,
    i.\,e., all clauses stored will be send to the solver.
  \item The formula and the solver will be kept in sync, that is,
    clauses added to the formula will directly be added to the solver.
  \item The method \JMethod{removeClause()} can not be called
    anymore.
  \item The method \JMethod{isSatisfiable()} can now be called.
\end{enumerate}

Once a solver was registered to the formula, we can check if there is
a satisfying assignment:
\begin{lstlisting}[language=Java]
  phi.isSatisfiable();
\end{lstlisting}
This method will use the SAT solver to solve the formula. The whole
API is incremental, so we can modify the formula between calls of
this method (which will be faster than recreating new formulas). A
typical scenario would look like:
\begin{lstlisting}[language=Java]
  while (phi.isSatisfiable()) {
    phi.addClause(...);
    ...
  }
\end{lstlisting}
Sometimes, we actually would like to remove clauses between calls
(which we are not allowed to do, as mentioned earlier). To overcome
this issue, most incremental SAT solvers support the concept of
\emph{assumptions}. An assumption is an unit clause that is added to
the solver for a single run. We can for instances say
$x_1=\mathrm{true}$ and check if the formula is satisfiable
\emph{under this assumption}. After a
call of \JMethod{isSatisfiable()}, all assumptions are removed.
To check if a formula is satisfiable under a set of assumptions,
simply add them to the method call:
\begin{lstlisting}[language=Java]
  phi.isSatisfiable(1, -3);
\end{lstlisting}
Once we have defined a formula and solved it using
\JMethod{isSatisfiable()}, we are most likely interested in an actual
satisfying model. A model is a mapping from the variables to boolean
values, i.\,e., a \JClass{Map<Integer, Boolean>} and can be obtained
with the following call:
\begin{lstlisting}[language=Java]
  Map<Integer, Boolean> model = phi.getModel();
  System.out.printf("Value of %d is %b\n", 1, model.get(1));
  System.out.printf("Value of %d is %b\n", 2, model.get(2));
  System.out.printf("Value of %d is %b\n", 3, model.get(3));
\end{lstlisting}
Note that we can only obtain a model after a call to
\JMethod{isSatisfiable()}, and only if this call has returned
true. Otherwise the code from above will throw an exception.

\subsection{Auxiliary Variables}
When we model a problem as CNF formula, we often need a lot of
additional variables, which do not directly model parts of the problem
(as vertex is selected or not), but that model structural things of
the formula (to allow us to write them in short CNF). These variables
arise a lot and will be added by different methods to the
formula. However, if we talk about the formula on a higher level, we
actually do not want these variables. For instance, we do not want
have variables in our model that we do not know.

\Jdrasil\ provides the concept of \emph{auxiliary variables} to mark
variables as helper variables, that are not directly connected to the
modeled problem. The variable $x_3$ can be marked as auxiliary with the
following command:
\begin{lstlisting}[language=Java]
  phi.markAuxiliary(3);
\end{lstlisting}
Once a variable $x$ is marked as being auxiliary, the following will
happen:
\begin{enumerate}
  \item The variable list of the formula will not contain $x$.
  \item A model will not contain an entry for $x$.
  \item The auxiliary variable list will contain $x$.
  \item The behavior of the formula, a registered SAT solver, and the
    satisfiability of the formula will \emph{not} change. The variable
    is still part of the formula.
\end{enumerate}

\subsection{Cardinality Constraint}
Many problems can naturally be encoded into an CNF~–~\emph{when} we can
restrict the number of variables that we are allowed to set to
true. For instance, a \emph{vertex cover} of a graph is a subset of
its vertices, such that every edge is incident to one of these
vertices. The graph at the border, for instance, has the vertex cover $\{2,4\}$.
\marginpar{
  \tikz[]\graph[spring electrical layout, node distance=0.75cm]{1--2--{3,4},4--{5,6}};
}
For a given graph $G=(V,E)$, it is easy to write down a formula that
states that the graph has a vertex cover:
\[
   \phi=\bigwedge_{\{u,v\}\in E}(x_u\vee x_v).
\]
However, as simple this formula is, as uninteresting it is as well:
every graph contains a vertex cover~–~just take all the vertices. To make
the problem interesting (and difficult), we have to restrict the
number of vertices that we are allowed to set to true. This is exactly
what a \emph{cardinality constraint} does.

\Jdrasil\ provides two ways to add cardinality constraints to a
formula. In both cases, we first of all need so specify the set of
variables (or literals) that we wish to restrict:
\begin{lstlisting}[language=Java]
  // build the formula
  Formula phi = new Formula();
  phi.addClause(1, -2, -3);
  phi.addClause(2, -3);
  phi.addClause(3);
  
  // define a set that we wish to restrict
  Set<Integer> vars = new HashSet<>();
  vars.add(1);
  vars.add(2);
  vars.add(3);
\end{lstlisting}
Note that $\phi$ is satisfiable and has only one model, which sets all
variables to true. So we get:

\codeWithOutput{phi.isSatisfiable()}{true}

We can now restrict the number of variables that are allowed to be set
to true, for instance, to $2$:
\begin{lstlisting}[language=Java]
  phi.addAtMost(2, vars);
\end{lstlisting}
We now obtain, as expected:

\codeWithOutput{phi.isSatisfiable()}{false}

In a similar manner, we can also enforce that a certain amount of
variables \emph{must be set}, but for our formula this has no effect:
\begin{lstlisting}[language=Java]
  phi.addAtLeast(2, vars);
\end{lstlisting}
Both methods, \detail{These methods use sequential counters and
  introduced \textcolor{jdrasil.fg}{$O(kn)$} auxiliary variables \emph{per call}.}
\JMethod{addAtMost} and \JMethod{addAtLeast}, add
clauses and auxiliary variables to the formula. This should be used
for cardinality constraints that are used only once, since these
methods will add these clauses for every call again (even if the set
of variables does not change).

However, when we solve an optimization problem, we often wish to add a
cardinality constraint for the same set of variables again and
again. For instances, a typical routine to solve vertex cover would
look like:
\begin{lstlisting}[language=Java]
  Formula phi = ...       // as in the example
  phi.registerSolver();

  Set<Integer> vars = ... // all variables
  int k = vars.size() - 1;

  phi.addAtMost(k, vars);
  while (phi.isSatisfiable()) {
    k = k - 1;
    phi.addAtMost(k, vars);
  }

  System.out.println(k+1);
\end{lstlisting}
In such an incremental setup, the methods from above are not optimal,
since they would add the similar auxiliary clauses and variables over
and over again. To overcome this, \Jdrasil\ also provides
\emph{incremental cardinality constraints}. The following ensures that
at least $3$ and at most $6$ variables of the set \JMethod{vars} is
set to true:
\begin{lstlisting}[language=Java]
  phi.addCardinalityConstraint(3,6,vars);
\end{lstlisting}
This \detail{This method uses sorting networks and introduces
  \textcolor{jdrasil.fg}{$O(n\log^2 n)$} auxiliary variables overall.}
method will add a lot of auxiliary variables and clauses (most of
the time more than a single call for of \JMethod{addAtMost}), however,
it will reuse this. More precisely, while the first call adds a lot of
structure to the formula, incremental calls will only add single
clauses. So for the algorithm from above, this method is way more
efficient. 

Finally, \Jdrasil\ provides a third possibility to use cardinality
constraints. While computing tree decompositions, we often deal with
instances of small tree width (as the usual use case is
parameterized complexity). If $k$ is much smaller than $n$, a sorting
network is asymptotically not optimal in sense of introduced auxiliary
variables. For such scenarios, the \JClass{Formula} class provides the
method \JMethod{addDecreasingAtMost}.\detail{This method uses a
  variant sequential counter as well. It introduces \textcolor{jdrasil.fg}{$O(kn)$} auxiliary
  variables overall.}This method can be seen as a compromise between
\JMethod{addAtMost} (which is simple, but static), and
\JMethod{addCardinalityConstraint} (which is complex, but can be
decreased and increased between solver calls). In contrast, the method
\JMethod{addDecreasingAtMost} is as simple as the first method, but
allows to be decreased between calls to the solver. It is, however,
only efficient for small values of $k$; and only works for decreasing
upper bounds (and not increasing lower bounds). Note that we can
interstate this as a parameterized SAT encoding, where $k$ is the
parameter.

\section{SAT4J}
The Java Library SAT4J\footnote{\url{http://www.sat4j.org}} is the
most advanced and complete \Lang{SAT}-library for the Java
platform. Although it is not the fastest solver available, it is one
of the most widespread solver, as it has a clean API and a good
documentation.

\Jdrasil\ implements core functionality of SAT4J completely over
reflections. This is done in the \emph{intern} class
\JClass{SAT4JSolver}, which is a
\JClass{ISATSolver}. If the SAT4J library is found in
\Jdrasil's classpath, this class will be used by
\JClass{Formula} (see~\ref{section:satFormula}) to find a
model. This is fully capsuled from the user, which only has to work
with the formula class. 

If SAT4J is available in the classpath,
\JMethod(canRegisterSATSolver()) of \JClass{Formula} will
return true, if SAT4J is also used (this depends on \Jdrasil\ and other
loaded upgrades), the method \JMethod{init()} will return the String
``SAT4J''. 

\subsection{Installation}
To use SAT4J in \Jdrasil, it is sufficient to download the core
library\footnote{\url{http://forge.objectweb.org/project/showfiles.php?group_id=228}}.
We can perform this step automatically with:
\begin{lstlisting}[language=bash]
  ./gradlew upgrade_sat4j
\end{lstlisting}
\subsection{Usage}
Just add the SAT4J core library to the classpath:
\begin{lstlisting}
  java -cp bin:build/upgrades/org.sat4j.core.jar jdrasil.Exact
\end{lstlisting}
Or use one of \Jdrasil's start scripts, which automatically looks for
upgrades in these locations:
\begin{lstlisting}
  ./tw-exact
\end{lstlisting}
Of course, the SAT4J library can be stored at another location as
well. Other than that, just work with the class \JClass{jdrasil.sat.Formula}
as we would otherwise.

\section{Native IPASIR Solver}

Today's most advanced SAT solvers are mostly implemented in
C/C++. \Jdrasil\ can use such ``native'' solver with the help of Javas
JNI-API\footnote{\url{https://docs.oracle.com/javase/7/docs/technotes/guides/jni/spec/jniTOC.html}}. 
To be as general as possible and, thus, to support as many SAT solvers
as possible, \Jdrasil\ implements the
IPASIR\footnote{\url{http://baldur.iti.kit.edu/sat-race-2015/downloads/ipasir.h}}
interface, which is the reversed acronym for ``Re-entrant Incremental
Satisfiability Application Program Interface''. This interface was
proposed and used in recent incremental SAT challenges. 

A solver that implements the IPASIR interface just has to implement
the following 9 functions:
\begin{lstlisting}[language=c]
  const char* ipasir_signature();
  void* ipasir_init();
  void ipasir_release(void* solver);
  void ipasir_add(void* solver, int lit_or_zero);
  void ipasir_assume(void* solver, int lit);
  int ipasir_solve(void* solver);
  int ipasir_val(void* solver, int lit);
  int ipasir_failed(void* solver, int lit);
  void ipasir_set_terminate(void* solver, 
                            void* state, 
                            int (*terminate)(void* state));
\end{lstlisting}
More details about what these functions should do can be found on the
website of recent SAT challenges. The interface is closely related to
the API of modern solvers as Lingeling or PicoSAT, so that such
solvers can easily be linked against IPASIR.

\Jdrasil\ can use an IPASIR solver as upgrade using the class
\JClass{NativeSATSolver}, which implements the interface
\JClass{ISATSolver} with native methods. Note how this
interface, which we also use for other solvers like SAT4J, is closely
related to IPASIR. 

The C interface of \JClass{NativeSATSolver} can be found
in \file{jdrasil_sat_NativeSATSolver.h}, which is
located in the corresponding subproject folder at \file{subprojects/upgrades/ipasir/}.\note{We can always create this file with
\lstinline[basicstyle=\footnotesize\codefamily]{./gradlew cinterface}} The
corresponding C/C++ implementation
\file{jdrasil_sat_NativeSATSolver.cpp} implements these methods and
maps them against \file{ipasir.h}. This implementation takes care of
keeping \Jdrasil\ and the actual solver in sync, allowing \Jdrasil\ to
use multiple ``instances'' of the solver, and allows \Jdrasil\ to kill
the solver. 

\subsection{Installation} 
To compile an IPASIR upgrade for \Jdrasil, we have to compile the JNI
implementation in the file \file{jdrasil_sat_NativeSATSolver.cpp}
into a dynamic library, which either should be be called
\file{libjdrasil_sat_NativeSATSolver.so} or, depending on you
operating system,
\file{libjdrasil_sat_NativeSATSolver.dylib}. In order to do so, we
have to link against an exiting implementation of an IPASIR solver.

In \file{subprojects/upgrades/ipasir} there is a \file{Makefile} that
compiles the C++-file under the assumption that there is a library
\file{libipasirsolver.dylib} in the same folder. This library should
implement the ipasir interface with an actual SAT solver.

\emph{Using Gradle:} To install a custom IPASIR solver, place a IPASIR compatible
implementation called \file{libipasirsolver.dylib} in
\file{subprojects/upgrades/ipasir} and run
\begin{lstlisting}[language=bash]
  ./gradlew upgrade_ipasir
\end{lstlisting}
in the root folder of \Jdrasil.

\Jdrasil\ is also shipped with two default IPASIR compatible SAT solvers:
glucose\footnote{\url{www.labri.fr/perso/lsimon/glucose/}} and lingeling\footnote{\url{www.fmv.jku.at/lingeling/}}. To upgrade \Jdrasil\ with these state of the
art solvers, use the following commands:
\begin{lstlisting}[language=bash]
  ./gradlew upgrade_glucose
  ./gradlew upgrade_lingeling
\end{lstlisting}
\subsection{Usage}
Once we have compiled \Jdrasil's IPASIR-JNI interface against an
IPASIR solver, i.\,e., once we have a library called
\file{libjdrasil_sat_NativeSATSolver.dylib}, we can \emph{upgrade}
\Jdrasil\ ``on the fly''. \Jdrasil\ will look for the SAT solver in
its library path (not to be confused with its class path), so the
following call will allow \Jdrasil\ to use the solver:
\begin{lstlisting}[language=bash]
  java -cp bin -Djava.library.path=build/upgrades jdrasil.Exact
\end{lstlisting}
Of course, the path can be set to any location, wherever the upgrade
is stored. Note that this only sets the path to location at which
\Jdrasil\ searches the upgrade. If, however, the upgrade is compiled
against other dynamic libraries, these libraries are searched in the
default system depending way (and not in the above specified path).

Alternatively, we can use one of \Jdrasil's premade runscripts:
\begin{lstlisting}[language=bash]
  ./tw-exact
\end{lstlisting}


\bibliography{manual}
\end{document}
