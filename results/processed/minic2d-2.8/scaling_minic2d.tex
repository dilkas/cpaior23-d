%% template-AutoScaling.tex
%% Author: Zongxu Mu, Yasha Pushak
%% This is the LaTeX template file for Empirical Scaling Analyser (ESA) v2.
%% ESA takes the template, replace variables with their corresponding values,
%%	and generates an output file named AutoScaling.tex.
%% You may compile this file alone without running ESA to see how the output looks like.
%% Variables are surrounded by ``@@''s
%% Supported variable names include:
%%	- algName, e.g., ``WalkSAT/SKC''
%%	- instName, e.g., ``random 3SAT at phase transition''
%%	- models, e.g., ``\\begin{itemize}\n\\item $Exp\\left[a,b\\right]\\left(n\\right)=a\\cdot b^{n}$ \\quad{}(2-parameter exponential);\n\\end{itemize}''
%%	- numBootstrapSamples, the number of bootstrap samples used in the analysis, e.g., 1000
%%	- numInsts, the number of instances used in the analysis, e.g., 1200
%%      - numInsts[Train,Test], the number of instances used as support/challenge data in the anlaysis, e.g., 600.
%%	- largestSupportSize, e.g., 500
%%	- table-Details-dataset, e.g., ``\\input{table_Details-dataset}''
%%	- table-Details-dataset, e.g., ``\\input{table_Details-dataset}''
%%	- table-Fitted-models, e.g., ``\\input{table_Fitted-models}''
%%	- figure-fittedModels, e.g., ``\\includegraphics[width=0.8\textwidth]{fittedModels_loglog}''
%%	- supportSizes, the sizes used for fitting the models, e.g., ``200, 250, 300, 350, 400, 450, 500''
%%	- challengeSizes, e.g., ``600, 700, 800, 900, 1000''
%%	- table-Bootstrap-intervals-of-parameters, e.g., ``\\input{table_Bootstrap-intervals-of-parameters}''
%%	- table-Bootstrap-intervals, e.g., ``\\input{table_Bootstrap-intervals}''
%%	- analysisSummary, e.g., ``observed median running times are consistent with the polynomial scaling model''
\documentclass[british]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3.5cm,bmargin=3.5cm,lmargin=3cm,rmargin=3cm}
\usepackage{array}
\usepackage{multirow}
\usepackage{amstext}
\usepackage{graphicx}
\usepackage[usenames, dvipsnames]{color}

\newcommand{\updatedYP}[1]{#1}
\newcommand{\yp}[1]{#1}
\newcommand{\orange}[1]{#1}
\newcommand{\evalModels}[1]{#1}
\newcommand{\bestBoot}[1]{#1}

\newcommand{\medianInterval}[1]{}
\newcommand{\randomizedAlgorithm}[1]{}
\newcommand{\quantileRegression}[1]{}
\renewcommand{\quantileRegression}[1]{#1}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

\title{On the empirical scaling of running time of minic2d for solving random instances with mu = 2.8}
\author{Empirical Scaling Analyzer}

\makeatother

\usepackage{babel}
\begin{document}
\maketitle %


\section{Introduction}

This is the automatically generated report on the empirical scaling
of the running time of minic2d for solving random instances with mu = 2.8.


\section{Methodology}

\label{sec:Methodology}

% models, model fitting
For our scaling analysis, we considered the following parametric models:
\begin{itemize} 
\item $Exp\left[a,b\right]\left(n\right)=a\times b^{n}$ \quad{}(2-parameter Exp)\item $Poly\left[a,b\right]\left(n\right)=a\times n^{b}$ \quad{}(2-parameter Poly)\end{itemize}
% \begin{itemize}
% \item $Exp\left[a,b\right]\left(n\right)=a\cdot b^{n}$ \quad{}(2-parameter exponential);
% \item $RootExp\left[a,b\right]\left(n\right)=a\cdot b^{\sqrt{n}}$ \quad{}(2-parameter root-exponential);
% \item $Poly\left[a,b\right]\left(n\right)=a\cdot n^{b}$ \quad{}(2-parameter polynomial).
% \end{itemize}
Note that the approach could be easily extended to other scaling models.
For fitting parametric scaling models to observed data, we used
\quantileRegression{%
iteratively re-weighted linear
}%
least squres
\quantileRegression{%
to perform quantile
}%
regression.
Since this method works best for linear models, we used
log transformations to convert
non-linear models into linear models. This transformation biases the
fitted models to more heavily favour the smaller training instance
sizes, so we used a heuristic error correction term to compensate.
Preliminary studies using this fitting method when applied to simulated
running time datasets with known scaling properties show that using the
heuristic error correction term improves the quality of the fitted models
and allows the procedure to fit scaling models consistent with the true,
underlying scaling of the data.

% what we fitted the models to, how we assessed model fit
The fitted models correspond to performance predictions for the empirical
scaling of the median of the distribution of running times.
\randomizedAlgorithm{%
Since minic2d is a randomized algorithm, we analyzed the per-instance
median of 1 independent runs for
each instance. This means that the model predictions correspond to
medians of
per-instance medians. Similarly, the running time statistics
reported throughout this report are statistics of per-instance
medians.
}%
%Compared to the mean, the median has two
%advantages: it is statistically more stable and immune to the presence
%of a certain amount of timed-out runs.
To assess the fit of a given
scaling model to observed data, we used the mean absolute error as a loss function.

\medianInterval{%
Due to the instances for which the running times are unknown,
there is uncertainty about the
precise location of the medians of the running time distributions
at each such $n$, and we can only provide bounds on
those medians instead. Closely following \cite{dubois2015on}, we calculate
these bounds based on the best-and worst-case scenarios, in which all instances with unknown running times
are easiest (required 0 running time) or hardest (required infinite running time), respectively.
We note that these are not confidence
intervals, since we can guarantee the actual median running
times to lie within them.
We also calculate model losses and confidence intervals based on these bounds.
}

% bootstrap confidence intervals
Closely following \cite{PusHoo20}, we
computed 95.0\% bootstrap confidence intervals for the performance predictions
obtained from our scaling models, based on 1001 bootstrap samples
per instance set and 1001 automatically fitted variants of each scaling
model.
\bestBoot{To extend this idea, we calculated training and challenge losses for
each of the fitted models' predictions and the corresponding
bootstrap samples of the observed data. We used these bootstrap sample
losses to calculate median and 95\% confidence intervals of the support and
challenge losses for each model.
\medianInterval{In order to handle the unknown running times, we used the geometric
means of the median intervals for each instance size to calculate the median losses.
However, to better capture both sources of uncertainty for the bootstrap intervals, we
calculated the loss interval upper and lower bounds by computing 97.5 and 2.5 quantiles
for the set of upper bounds and the set of lower bounds on the losses, respectively.}}%
\randomizedAlgorithm{Since this analysis was performed on per-instance
medians, we also computed these statistics on nested, per-instance
bootstrap samples, by first computing medians for
21 bootstrap samples for each instance and then
randomly selecting one of the per-instance medians when needed.}

We calculated the observed point estimates for the medians of the
data by fitting a linear model to local data with Guassian weights, and
then recording the observed statistic as the prediction from the linear
model as the mid-point of the local data.
\updatedYP{%
In the following, we say that a scaling model prediction is in-consistent
with observed data if the bootstrap confidence interval for the observed data
is disjoint from the bootstrap confidence interval for the
predicted median \randomizedAlgorithm{of per-instance
median}
running times; we say that a scaling model prediction is weakly consistent
with the observed data if the bootstrap confidence interval for the prediction
overlaps with the bootstrap confidence interval for the observed data;
%\medianInterval{we say that a scaling model prediction is consistent
%with observed data, if the interval for observed median \randomizedAlgorithm{of per-instance median} running times
%overlaps with the
%bootstrap confidence interval for predicted running times;}
and, we say that a scaling model is strongly consistent with observed
data, if the bootstrap confidence interval for the observed median
\randomizedAlgorithm{of per-instance medians} is fully contained
within the bootstrap confidence interval for predicted
running times.}
Also, we define the residue of a model at a given size as the observed
point estimate less the
predicted value using the fitted running time scaling model (fitted to
the set of training data).


\section{Dataset Description}

The dataset contains running times of the minic2d algorithm solving
5100 instances of different sizes \randomizedAlgorithm{with
1 independent runs per instance}. We split the
running times into two categories, \emph{support} or \emph{training}
instances ($n\leq35$) and \emph{challenge} or
\emph{test} instances ($n>35$) with 1754
and 3346 instances, respectively. The
details of the dataset can be found in Tables \ref{tab:Details-dataset-support}
and \ref{tab:Details-dataset-challenge}.
\begin{table*}
\noindent \begin{centering}
\input{table_Details-dataset-support}
\par\end{centering}

\caption{\label{tab:Details-dataset-support} Details of the running time
dataset used as support data for model fitting.
\randomizedAlgorithm{The reported statistics are of the per-instance
median running times.} The ``\# of instances'' is the
sum of the weights of the instances used to calculate these statistics.}
\end{table*}

\begin{table*}
\noindent \begin{centering}
\input{table_Details-dataset-challenge}
\par\end{centering}

\caption{\label{tab:Details-dataset-challenge} Details of the running time
dataset used as challenge data for model fitting.
\randomizedAlgorithm{The reported statistics are of the per-instance
median running times.} The ``\# of instances'' is the
sum of the weights of the instances used to calculate these statistics.}
\end{table*}

%
% Figure \ref{fig:CDFs} shows the distributions of the running times of
% minic2d solving random instances with mu = 2.8.
% \begin{figure*}[tb]
% \begin{centering}
% \includegraphics[width=0.8\textwidth]{cdfs}
% % \includegraphics[width=0.8\textwidth]{cdfs}
% \par\end{centering}
%
% \noindent \centering{}\caption{\label{fig:CDFs} Distribution of running times across instance sets for
% minic2d.}
% \end{figure*}
%
%

\section{Empirical Scaling of Solver Performance}

\label{sec:Results}

We first fitted our parametric scaling models to the
medians of the
\randomizedAlgorithm{per-instance median} running times
of minic2d, as described in Section \ref{sec:Methodology}. The
models were fitted using the training instance data and later
challenged with the test instance data.
This resulted in the models, shown along with losses on support and
challenge data, shown in Table~\ref{tab:Fitted-models}.
\begin{table}[tb]
\begin{centering}
\input{table_Fitted-models}
% \input{table_Fitted-models}
\par\end{centering}

\caption{\label{tab:Fitted-models}Fitted models of the medians
of the \randomizedAlgorithm{per-instance median} running
times and loss
values (in CPU sec). The models yielding the most
accurate predictions (as per losses on challenge data) are shown in
boldface.}
\end{table}
In addition, we illustrate the fitted models of minic2d in
Figure~\ref{fig:Fitted-models},
and the residues for the models in Figure~\ref{fig:Fitted-residues}.
\begin{figure}[tb]
\noindent \begin{centering}
\includegraphics[width=0.8\textwidth]{fittedModels}
% \includegraphics[width=0.8\textwidth]{fittedModels}
\par\end{centering}

\caption{\label{fig:Fitted-models} Fitted models of the medians of the
\randomizedAlgorithm{per-instance median} running times.
The models correspond to predictions for the medians of the
\randomizedAlgorithm{per-instance median} running times of
minic2d solving the set of random instances with mu = 2.8
with $27\leq n\leq 35$ variables, and are challenged by the medians of
the \randomizedAlgorithm{per-instance median}
running times of $35< n \leq 44$ variables.}
\end{figure}


\begin{figure}[tb]
\noindent \begin{centering}
\includegraphics[width=0.8\textwidth]{fittedResidues}
% \includegraphics[width=0.8\textwidth]{fittedResidues}
\par\end{centering}

\caption{\label{fig:Fitted-residues} Residues of the fitted models of the
medians of the \randomizedAlgorithm{per-instance median}
running times. }
\end{figure}


But how much confidence should we have in these models? Are the losses
small enough that we should accept them? To answer this question,
we assessed the fitted models using the bootstrap approach outlined
in Section~\ref{sec:Methodology}. Table~\ref{tab:Bootstrap-intervals-of-parameters}
shows the bootstrap intervals of the model parameters,
\bestBoot{Table~\ref{tab:Bootstrap-model-RMSE}
shows the bootstrap intervals of the model prediction losses},
and Table~\ref{tab:Bootstrap-intervals-support}
contains the bootstrap intervals for the support data.
Challenging the models with extrapolation, as shown in
\evalModels{Table~\ref{tab:Bootstrap-intervals-challenge}, it is concluded that
the Exp model tends to under-estimate the data, and the Poly model tends to under-estimate the data
(as also illustrated in Figure~\ref{fig:Fitted-models}).
We base these statements on an analysis of the fraction of predicted bootstrap intervals that are strongly consistent, weakly consistent and disjoint from the observed bootstrap intervals for the challenge data. To provide stronger emphasis for the largest instance sizes, we also consider these fractions for the largest half of the challenge instance sizes. To be precise, ; and we say a model tends to under-estimate the data if $> 10\%$ of the confidence intervals for predictions on challenge instance sizes are disjoint from the confidence intervals for observed running time data and $\geq 90\%$ of the predicted intervals are below or are consistent with the observed intervals. }
\begin{table*}[tb]
\noindent \begin{centering}
\input{table_Bootstrap-intervals-of-parameters}
% \input{table_Bootstrap-intervals-of-parameters}

\par\end{centering}
\caption{\label{tab:Bootstrap-intervals-of-parameters} 95\% bootstrap intervals
of model parameters for the medians of the
\randomizedAlgorithm{per-instance median} running times}

%Group tables 4 and 5 together.
%\end{table*}
%\begin{table*}[tb]
\bigskip

\noindent \begin{centering}
\input{table_Bootstrap-model-loss}:
% \input{table_Bootstrap-model-Loss}

\par\end{centering}
\caption{\label{tab:Bootstrap-model-RMSE} \bestBoot{95\% bootstrap confidence
intervals
of model prediction losses for the medians of the
\randomizedAlgorithm{per-instance median} running times.%
%\medianInterval{To calculate the median RMSEs we used the geometric mean of the intervals for the medians of
%the \randomizedAlgorithm{per-instance median} running times for each instance size. However, the bootstrap
%confidence intervals directly capture both sources of uncertainty by reporting the 2.5 and 97.5
%quantiles of the lower and upper bounds on the RMSEs, respectively.}
}}
\end{table*}


\begin{table*}[tb]
\noindent \begin{centering}
\input{table_Bootstrap-intervals_support}
% \input{table_Bootstrap-intervals}
\par\end{centering}

\caption{\label{tab:Bootstrap-intervals-support} 95\% bootstrap confidence
intervals
for the medians of the
\randomizedAlgorithm{per-instance median} running time
predictions and observed running times on random instances with mu = 2.8.
The instance sizes shown here are those used for fitting the models.
Bootstrap intervals on predictions that are weakly consistent
with the observed point estimates are shown in boldface
%\medianInterval{those that are consistent are marked by plus signs ({+}),}
and those that are strongly consistent are marked
by asterisks ({*}).}
%and those that fully contain the confidence intervals on
%observations are marked by asterisks ({*}).}
\end{table*}

\begin{table*}[tb]
\noindent \begin{centering}
\input{table_Bootstrap-intervals_challenge}
% \input{table_Bootstrap-intervals}
\par\end{centering}

\caption{\label{tab:Bootstrap-intervals-challenge} 95\% bootstrap confidence intervals
for the medians of the \randomizedAlgorithm{per-instance median}
running time predictions and observed running times on random instances with mu = 2.8.
The instance sizes shown here are larger than those used for fitting the models.
Bootstrap intervals on predictions that are weakly consistent
with the observed data are shown in boldface
%\medianInterval{those that are consistent are marked by plus signs ({+}),}
and those that are strongly consistent are marked
by asterisks ({*}).}
%and those that fully contain the confidence intervals on
%observations are marked by asterisks ({*}).}
\end{table*}


\section{Conclusion}

In this report, we presented an empirical analysis of the scaling
behaviour of minic2d on random instances with mu = 2.8. We found
the Exp model tends to under-estimate the data, and the Poly model tends to under-estimate the data.

\bibliographystyle{plain}
\begin{thebibliography}{1}

\bibitem{PusHoo20}
Yasha Pushak and Holger~H. Hoos.
\newblock Advanced Statistical Analysis of Empirical Performance Scaling.
\newblock in {\em Proceedings of the 22nd Genetic and Evolutionary Computation Conference}, (GECCO 2020), pages 236--244, 2020.

\bibitem{PusEtAl20}
Yasha Pushak, Zongxu Mu and Holger~H. Hoos.
\newblock Empirical scaling analyzer: An automated system for empirical analysis of performance scaling.
\newblock {\em AI Communications} -- to appear, 2020.

\bibitem{dubois2015on}
J{\'e}r{\'e}mie Dubois-Lacoste, Holger~H. Hoos, and Thomas St{\"u}tzle.
\newblock On the empirical scaling behaviour of state-of-the-art local search
algorithms for the {E}uclidean {TSP}.
\newblock In {\em Proceedings of the 17th Genetic and Evolutionary Computation Conference}, (GECCO 2015), pages 377--384, 2015.

\bibitem{hoos2009bootstrap}
Holger~H. Hoos.
\newblock A bootstrap approach to analysing the scaling of empirical run-time
data with problem size.
\newblock Technical report, Technical Report TR-2009-16, Department of Computer Science, University of British
Columbia, 2009.

\bibitem{hoos2014empirical}
Holger~H. Hoos and Thomas St{\"u}tzle.
\newblock On the empirical scaling of run-time for finding optimal solutions to
the travelling salesman problem.
\newblock {\em European Journal of Operational Research}, 238(1):87--94, 2014.

\end{thebibliography}

\end{document}
